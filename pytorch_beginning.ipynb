{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, ConcatDataset\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最初に、非常に長いので、IPython-notebook-extensionを導入し、Table of Contentsを有効化することをおすすめします。<br>\n",
    "詳しくは、<br>\n",
    "https://cartman0.hatenablog.com/entry/2016/03/28/170319\n",
    "<br>などを参考にしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorchの主なパッケージ\n",
    "torch<br>\n",
    "torch.nn<br>\n",
    "torch.optim<br>\n",
    "torch.autograd<br>\n",
    "torch.utils<br>\n",
    "torch.multiprocessing<br>\n",
    "torchvision<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テンソル\n",
    "### テンソルとは\n",
    "pytorchではTensorという型で演算を行います。<br>\n",
    "Tensorはその名の通りテンソル、つまり多次元配列を扱うためのデータ構造です。<br>\n",
    "NumPyのndarrayとほぼ同様のAPIを有しており、それに加えてGPUによる計算もサポートしています。<br>\n",
    "Tensorは各データ型に対して定義されており、例えば32bitの浮動小数点数でしたらtorch.FloatTensorを、64bitの符号付き整数でしたらtorch.LongTensorを使用します。<br>\n",
    "\n",
    "また、GPU上で計算する場合はtorch.cuda.FloatTensorなどを使用します。なお、TensorはFloatTensorのエイリアスです。いずれの型のTensorでもtorch.tensorという関数で作成することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t1 = torch.tensor([[1,2,3],[4,5,6]])\n",
    "#引数のリストの値で初期化したテンソルを作成します.\n",
    "t2 = torch.empty(5, 3)\n",
    "t3 = torch.rand(5, 3)\n",
    "t4 = torch.zeros(5, 3, dtype=torch.long)\n",
    "t5 = torch.FloatTensor(2,3,4)\n",
    "#(2 x 3 x 4)のテンソルで初期化されていないものを作成しています。pytorchでは最初の２がチャネル数を意味するので注意してください。\n",
    "t6 = torch.randn(3,4,5)\n",
    "#(3 x 4 x 5)のテンソルで平均０分散１の正規分布で初期化したものを作成しています"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### インプレース/アウトオブプレース\n",
    "最後に_をつけると、元の変数の内容が変わります."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 3, 3],\n",
      "        [3, 3, 3]])\n",
      "tensor([[3, 3, 3],\n",
      "        [3, 3, 3]])\n",
      "tensor([[6, 6, 6],\n",
      "        [6, 6, 6]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([[1,2,3],[4,5,6]])\n",
    "t.fill_(3.5)\n",
    "print(t)\n",
    "t.add(3.5)\n",
    "print(t)\n",
    "t.add_(3.5)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### インデキシング\n",
    "以下のようにnumpyと同様のインデキシング操作をサポートしています."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([[1,2,3], [4,5,6.]])\n",
    "\n",
    "# スカラーの添字で指定\n",
    "t[0, 2]\n",
    "\n",
    "# スライスで指定\n",
    "t[:, :2]\n",
    "\n",
    "# 添字のリストで指定\n",
    "t[:, [1,2]]\n",
    "\n",
    "# マスク配列を使用して3より大きい部分のみ選択\n",
    "t[t > 3]\n",
    "\n",
    "# [0, 1]要素を100にする\n",
    "t[0, 1] = 100\n",
    "\n",
    "# スライスを使用した一括代入\n",
    "t[:, 1] = 200\n",
    "\n",
    "# マスク配列を使用して特定条件の要素のみ置換\n",
    "t[t > 10] = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テンソルの演算\n",
    "四則演算はTensor同士かTensorとPythonのスカラーの数値との間でのみ可能です。<br>\n",
    "Tensorとndarrayとでは演算がサポートされていないことに注意してください。\n",
    "\n",
    "　また、Tensor同士の場合も同じ型である必要があります。<br>\n",
    " 例えばFloatTensorとDoubleTensorの演算はエラーになります。<br>\n",
    " 四則演算はndarrayと同様にブロードキャストが適用され、ベクトルとスカラーや行列とベクトル間の演算でも自動的に次元が補間されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 長さ3のベクトル\n",
    "v = torch.tensor([1, 2, 3.])\n",
    "w = torch.tensor([0, 10, 20.])\n",
    "# 2 × 3の行列\n",
    "m = torch.tensor([[0, 1, 2], [100, 200, 300.]])\n",
    "\n",
    "# ベクトルとスカラーの足し算\n",
    "v2 = v + 10\n",
    "# 累乗も同様\n",
    "v2 = v ** 2\n",
    "# 同じ長さのベクトル同士の引き算\n",
    "z = v - w\n",
    "# 複数の組み合わせ\n",
    "u = 2 * v - w / 10 + 6.0\n",
    "\n",
    "# 行列とスカラー\n",
    "m2 = m * 2.0\n",
    "# 行列とベクトル\n",
    "#(2, 3)の行列と(3,)のベクトルなのでブロードキャストが働く\n",
    "m3 = m + v\n",
    "# 行列同士\n",
    "m4 = m + m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数学関数\n",
    "演算子<br>\n",
    "・dot :ベクトルの内積<br>\n",
    "・mv :行列とベクトルの積<br>\n",
    "・mm :行列と行列の積<br>\n",
    "・matmal :引数の種類によって自動的にdot,mv,mmを選択して実行<br>\n",
    "・gsev :LU分解による連立方程式の解<br>\n",
    "・eig,symeig :固有値分解. symeigは対称行列用のより効率のよいアルゴリズム<br>\n",
    "・svd :特異値分解<br>\n",
    "\n",
    "その他の関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 × 10のテストデータを用意\n",
    "X = torch.randn(100, 10)\n",
    "\n",
    "# 数学関数を含めた数式\n",
    "y = X * 2 + torch.abs(X)\n",
    "\n",
    "# 平均値を求める\n",
    "m = torch.mean(X)\n",
    "# 関数ではなく、メソッドとしても利用できる\n",
    "m = X.mean()\n",
    "# 集計結果は0次元のTensorでitemメソッドを使用して、\n",
    "# 値を取り出すことができる\n",
    "m_value = m.item()\n",
    "# 集計は次元を指定できる。以下は行方向に、\n",
    "# 集計して列ごとに平均値を計算している\n",
    "m2 = X.mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### インデキシング操作関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.tensor([[1, 2], [3, 4.]]) # 2×2\n",
    "x2 = torch.tensor([[10, 20, 30], [40, 50, 60.]]) # 2×3\n",
    "\n",
    "# 2×2を4×1に見せる\n",
    "x1.view(4, 1)\n",
    "# -1は残りの次元を表し、一度だけ使用できる\n",
    "# 以下の例では-1とすると自動的に4になる\n",
    "x1.view(1, -1)\n",
    "\n",
    "# 2×3を転置して3×2にする\n",
    "x2.t()\n",
    "\n",
    "# dim=1に対して結合することで2×5のTensorを作る\n",
    "torch.cat([x1, x2], dim=1)\n",
    "\n",
    "# 画像データの形式をHWC(縦、横、色)をCHWCHW(色、縦、横)に変換\n",
    "# 64×32×3のデータが100個\n",
    "hwc_img_data = torch.rand(100, 64, 32, 3)\n",
    "chw_img_data = hwc_img_data.transpose(1, 2).transpose(1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU の使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-95817b84fe4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Pytorch 0.4 以降\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# cudaの後に:数字で対応したGPUを使用\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m         raise RuntimeError(\n\u001b[1;32m    160\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0m_cudart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_isDriverSufficient'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_isDriverSufficient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_getDriverVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(10)\n",
    "y = torch.randn(10)\n",
    "#Pytorch 0.4 以降\n",
    "x = x.to('cuda')\n",
    "y = y.to('cuda:0') # cudaの後に:数字で対応したGPUを使用\n",
    "\n",
    "z = x * y\n",
    "\n",
    "z = z.to('cpu') # cpuへ\n",
    "print(x.is_cuda) # 変数がGPU上にあればTrue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorとnumpyの相互変換\n",
    "Tensor -> numpy ： (Tensorの変数).numpy()<br>\n",
    "numpy -> Tensor ： torch.from_numpy(Numpyの変数)<br>\n",
    "ただし、GPU上のTensorはそのままでは変換できず、一度CPU上に移す必要があります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f53b2e739752>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# GPU上のTensorはcpuメソッドで、\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 一度CPUのTensorに変換する必要がある\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m         raise RuntimeError(\n\u001b[1;32m    160\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0m_cudart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_isDriverSufficient'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_isDriverSufficient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_getDriverVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# numpyメソッドを使用してndarrayに変換\n",
    "t = torch.tensor([[1, 2], [3, 4.]])\n",
    "x = t.numpy()\n",
    "\n",
    "# GPU上のTensorはcpuメソッドで、\n",
    "# 一度CPUのTensorに変換する必要がある\n",
    "t = torch.tensor([[1, 2], [3, 4.]], device=\"cuda:0\")\n",
    "x = t.to(\"cpu\").numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの取得\n",
    "pytorchで与えるデータはTensor(train), Tensor(target)のようにする必要がある。<br>\n",
    "データラベルを同時に変換させる関数としてTensorDatasetがある。pytorchのDataLoaderはバッチ処理のみ対応している。\n",
    "### transforms/Dataset/DataLoader\n",
    "学習時のデータの前処理および、データの(バッチ毎の)ロードを行うためにtransforms/Dataset/DataLoaderが用意されている。大まかな役割としては、<br>\n",
    "・transform : データの前処理を担当するモジュール<br>\n",
    "・Dataset : データとそれに対応するラベルを1組返すモジュール。データを返すときにtransformを使って前処理を行う。<br>\n",
    "・DataLoader : データセットからデータをバッチサイズに固めて返すモジュール<br>\n",
    "となっている。以下で順に例を用いて説明する。\n",
    "#### transforms\n",
    "transformsはデータの前処理を行う部分であり、CropやFlipなどの画像処理においてよく用いられるものは、torchvision.transformsに入っているが、これを自分で好きなように実装することもできる。<br>\n",
    "ただし、自分で実装する際には、__call__を実装した、「コール可能なクラス」として実装する必要がある。<br>\n",
    "※ 因みに、__init__はインスタンス生成時に実行し、一度生成したインスタンスを引数を与え関数のように呼び出せば__init__と同時に__call__が実行される。<br>\n",
    "callを用いたクラスとして実装することで、インスタンス化時に前処理に使うパラメータを全部渡しておけるので、前処理を実行するたびにパラメータを渡す手間が省ける、ということで「コール可能なクラス」を推奨している(らしい)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Square(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self,sample):\n",
    "        return sample**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "Datasetは入力データとそれに対応するラベルを1組返すモジュール。<br>\n",
    "PyTorchでは有名なデータセットがあらかじめtorchvision.datasetsに定義されている。(MNIST/CIFAR/STL10など)<br>\n",
    "扱うデータが画像でクラスごとにフォルダ分けされている場合はtorchvision.datasets.ImageFolderという便利なクラスもある。(KerasのImageDataGeneratorのflow_from_directory()のような機能)<br>\n",
    "\n",
    "Datasetを実装するのに必要な要件<br>\n",
    "オリジナルDatasetを実装するときに守る必要がある要件は以下３つ。<br>\n",
    "・torch.utils.data.Datasetを継承する。<br>\n",
    "・__len__を実装する。<br>\n",
    "・__getitem__を実装する。<br>\n",
    "__len__は、len(obj)で実行されたときにコールされる関数。<br>\n",
    "__getitem__は、obj[i]のようにインデックスで指定されたときにコールされる関数。<br>\n",
    "今回は、データは数字のリスト、ラベルは偶数の場合だけTrueになるものを出力するDatasetを実装する。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, data_num, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data_num = data_num\n",
    "        self.data = []\n",
    "        self.label = []\n",
    "        for x in range(self.data_num):\n",
    "            self.data.append(x) # 0 から (data_num-1) までのリスト\n",
    "            self.label.append(x%2 == 0) # 偶数ならTrue 奇数ならFalse\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_num\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_data = self.data[idx]\n",
    "        out_label =  self.label[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            out_data = self.transform(out_data)\n",
    "\n",
    "        return out_data, out_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoader\n",
    "データセットからデータをバッチサイズに固めて返すモジュール<br>\n",
    "DataLoaderはデータセットを使ってバッチサイズ分のデータを生成する。<br>\n",
    "またデータのシャッフル機能も持つ。<br>\n",
    "データを返すときは、データをtensor型に変換して返す。<br>\n",
    "DataLoaderは、torch.utils.data.DataLoaderというクラスが既に用意されている。たいていの場合、このクラスで十分対応できるので、今回はこのクラスにこれまで実装してきたDatasetを渡して動作を見てみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([100, 121,  49,  81]), tensor([1, 0, 0, 0])]\n",
      "[tensor([169, 196,  36,   1]), tensor([0, 1, 1, 0])]\n",
      "[tensor([ 64,   4, 256,   9]), tensor([1, 1, 1, 0])]\n",
      "[tensor([324, 289, 144, 225]), tensor([1, 0, 1, 0])]\n",
      "[tensor([361,  16,   0,  25]), tensor([0, 1, 1, 0])]\n"
     ]
    }
   ],
   "source": [
    "data_set = MyDataset(20, transform=Square())\n",
    "dataloader = torch.utils.data.DataLoader(data_set, batch_size=4, shuffle=True)\n",
    "\n",
    "for i in dataloader:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFARのロードとDataLoader\n",
    "CIFARなどのデータセットはtorchvisionに実装されている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "torch.Size([3, 32, 32])\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "#train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
    "                                             train=True, \n",
    "                                             transform=transforms.ToTensor(),\n",
    "                                             download=True)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "# Fetch one data pair (read data from disk).\n",
    "image, label = train_dataset[0]\n",
    "print (image.size())\n",
    "print (label)\n",
    "\n",
    "# Data loader (this provides queues and threads in a very simple way).\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "# When iteration starts, queue and thread start to load data from files.\n",
    "data_iter = iter(train_loader)\n",
    "\n",
    "# Mini-batch images and labels.\n",
    "images, labels = data_iter.next()\n",
    "\n",
    "# Actual usage of the data loader is as below.\n",
    "for images, labels in train_loader:\n",
    "    # Training code should be written here.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shuffle=Trueなどとすることであらかじめシャッフルしておくことも可能<br><br>\n",
    "また、torchvisionを使って画像処理関連処理(transformでTensor化、標準化、クロップなどの処理を一律に実行可能)、<br>\n",
    "CIFAR-10などのデータセットの読み込みができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/170498071 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 8192/170498071 [00:00<1:02:58, 45126.05it/s]\u001b[A\n",
      "  0%|          | 40960/170498071 [00:01<49:53, 56939.82it/s] \u001b[A\n",
      "  0%|          | 73728/170498071 [00:01<39:35, 71733.60it/s]\u001b[A\n",
      "  0%|          | 139264/170498071 [00:01<30:03, 94461.60it/s]\u001b[A\n",
      "  0%|          | 204800/170498071 [00:01<23:23, 121355.98it/s]\u001b[A\n",
      "  0%|          | 270336/170498071 [00:01<18:45, 151217.05it/s]\u001b[A\n",
      "  0%|          | 335872/170498071 [00:02<15:28, 183235.31it/s]\u001b[A\n",
      "  0%|          | 401408/170498071 [00:02<12:19, 229952.16it/s]\u001b[A\n",
      "  0%|          | 466944/170498071 [00:02<10:58, 258225.73it/s]\u001b[A\n",
      "  0%|          | 532480/170498071 [00:02<10:01, 282647.75it/s]\u001b[A\n",
      "  0%|          | 614400/170498071 [00:02<09:41, 292251.36it/s]\u001b[A\n",
      "  0%|          | 679936/170498071 [00:02<09:07, 310255.04it/s]\u001b[A\n",
      "  0%|          | 745472/170498071 [00:03<07:46, 363929.77it/s]\u001b[A\n",
      "  0%|          | 794624/170498071 [00:03<07:16, 388579.01it/s]\u001b[A\n",
      "  0%|          | 843776/170498071 [00:03<08:14, 343421.72it/s]\u001b[A\n",
      "  1%|          | 892928/170498071 [00:03<08:18, 340013.12it/s]\u001b[A\n",
      "  1%|          | 958464/170498071 [00:03<08:09, 346498.15it/s]\u001b[A\n",
      "  1%|          | 1024000/170498071 [00:03<07:15, 389444.90it/s]\u001b[A\n",
      "  1%|          | 1073152/170498071 [00:03<06:55, 407495.39it/s]\u001b[A\n",
      "  1%|          | 1122304/170498071 [00:04<07:46, 363245.40it/s]\u001b[A\n",
      "  1%|          | 1187840/170498071 [00:04<07:29, 376842.43it/s]\u001b[A\n",
      "  1%|          | 1236992/170498071 [00:04<07:09, 394443.07it/s]\u001b[A\n",
      "  1%|          | 1302528/170498071 [00:04<06:28, 435591.06it/s]\u001b[A\n",
      "  1%|          | 1351680/170498071 [00:04<06:57, 405330.87it/s]\u001b[A\n",
      "  1%|          | 1400832/170498071 [00:04<06:44, 418199.21it/s]\u001b[A\n",
      "  1%|          | 1466368/170498071 [00:04<06:13, 452105.93it/s]\u001b[A\n",
      "  1%|          | 1515520/170498071 [00:05<06:41, 421050.37it/s]\u001b[A\n",
      "  1%|          | 1597440/170498071 [00:05<05:51, 480031.29it/s]\u001b[A\n",
      "  1%|          | 1654784/170498071 [00:05<05:50, 481197.54it/s]\u001b[A\n",
      "  1%|          | 1712128/170498071 [00:05<06:07, 458801.31it/s]\u001b[A\n",
      "  1%|          | 1794048/170498071 [00:05<05:25, 518853.37it/s]\u001b[A\n",
      "  1%|          | 1851392/170498071 [00:05<05:32, 507213.55it/s]\u001b[A\n",
      "  1%|          | 1925120/170498071 [00:05<05:28, 513909.27it/s]\u001b[A\n",
      "  1%|          | 2007040/170498071 [00:05<04:52, 575078.80it/s]\u001b[A\n",
      "  1%|          | 2072576/170498071 [00:05<05:01, 558399.14it/s]\u001b[A\n",
      "  1%|▏         | 2154496/170498071 [00:06<04:56, 568206.11it/s]\u001b[A\n",
      "  1%|▏         | 2252800/170498071 [00:06<04:19, 647552.11it/s]\u001b[A\n",
      "  1%|▏         | 2326528/170498071 [00:06<04:14, 661281.68it/s]\u001b[A\n",
      "  1%|▏         | 2416640/170498071 [00:06<04:28, 625355.24it/s]\u001b[A\n",
      "  1%|▏         | 2531328/170498071 [00:06<03:53, 718789.10it/s]\u001b[A\n",
      "  2%|▏         | 2629632/170498071 [00:06<03:50, 727139.41it/s]\u001b[A\n",
      "  2%|▏         | 2711552/170498071 [00:06<04:04, 686664.78it/s]\u001b[A\n",
      "  2%|▏         | 2875392/170498071 [00:07<03:45, 744539.35it/s]\u001b[A\n",
      "  2%|▏         | 3022848/170498071 [00:07<03:12, 869011.15it/s]\u001b[A\n",
      "  2%|▏         | 3121152/170498071 [00:07<03:22, 824543.52it/s]\u001b[A\n",
      "  2%|▏         | 3235840/170498071 [00:07<03:19, 839555.50it/s]\u001b[A\n",
      "  2%|▏         | 3383296/170498071 [00:07<02:53, 962042.88it/s]\u001b[A\n",
      "  2%|▏         | 3497984/170498071 [00:07<02:48, 988984.65it/s]\u001b[A\n",
      "  2%|▏         | 3645440/170498071 [00:07<02:50, 980821.07it/s]\u001b[A\n",
      "  2%|▏         | 3809280/170498071 [00:07<02:32, 1090152.37it/s]\u001b[A\n",
      "  2%|▏         | 3940352/170498071 [00:07<02:25, 1141479.38it/s]\u001b[A\n",
      "  2%|▏         | 4104192/170498071 [00:08<02:29, 1116644.66it/s]\u001b[A\n",
      "  3%|▎         | 4284416/170498071 [00:08<02:15, 1230592.10it/s]\u001b[A\n",
      "  3%|▎         | 4448256/170498071 [00:08<02:11, 1258821.00it/s]\u001b[A\n",
      "  3%|▎         | 4612096/170498071 [00:08<02:11, 1262062.36it/s]\u001b[A\n",
      "  3%|▎         | 4825088/170498071 [00:08<01:58, 1394083.63it/s]\u001b[A\n",
      "  3%|▎         | 5005312/170498071 [00:08<01:56, 1415030.13it/s]\u001b[A\n",
      "  3%|▎         | 5185536/170498071 [00:08<01:57, 1411781.60it/s]\u001b[A\n",
      "  3%|▎         | 5447680/170498071 [00:08<01:43, 1591150.84it/s]\u001b[A\n",
      "  3%|▎         | 5619712/170498071 [00:09<01:41, 1624825.07it/s]\u001b[A\n",
      "  3%|▎         | 5840896/170498071 [00:09<01:45, 1553467.75it/s]\u001b[A\n",
      "  4%|▎         | 6135808/170498071 [00:09<01:32, 1778995.72it/s]\u001b[A\n",
      "  4%|▎         | 6332416/170498071 [00:09<01:36, 1701044.76it/s]\u001b[A\n",
      "  4%|▍         | 6578176/170498071 [00:09<01:33, 1759372.82it/s]\u001b[A\n",
      "  4%|▍         | 6922240/170498071 [00:09<01:21, 2010788.86it/s]\u001b[A\n",
      "  4%|▍         | 7143424/170498071 [00:09<01:26, 1895352.39it/s]\u001b[A\n",
      "  4%|▍         | 7397376/170498071 [00:09<01:23, 1962800.91it/s]\u001b[A\n",
      "  4%|▍         | 7643136/170498071 [00:10<01:18, 2081089.86it/s]\u001b[A\n",
      "  5%|▍         | 7905280/170498071 [00:10<01:13, 2210653.20it/s]\u001b[A\n",
      "  5%|▍         | 8249344/170498071 [00:10<01:05, 2472407.30it/s]\u001b[A\n",
      "  5%|▍         | 8519680/170498071 [00:10<01:09, 2345266.04it/s]\u001b[A\n",
      "  5%|▌         | 8806400/170498071 [00:10<01:07, 2384106.10it/s]\u001b[A\n",
      "  5%|▌         | 9068544/170498071 [00:10<01:05, 2450055.92it/s]\u001b[A\n",
      "  6%|▌         | 9428992/170498071 [00:10<01:00, 2663828.90it/s]\u001b[A\n",
      "  6%|▌         | 9773056/170498071 [00:10<00:56, 2848344.91it/s]\u001b[A\n",
      "  6%|▌         | 10076160/170498071 [00:10<00:56, 2856042.79it/s]\u001b[A\n",
      "  6%|▌         | 10477568/170498071 [00:11<00:55, 2871974.64it/s]\u001b[A\n",
      "  6%|▋         | 10772480/170498071 [00:11<00:55, 2859022.13it/s]\u001b[A\n",
      "  7%|▋         | 11190272/170498071 [00:11<00:50, 3157887.59it/s]\u001b[A\n",
      "  7%|▋         | 11526144/170498071 [00:11<00:49, 3214625.97it/s]\u001b[A\n",
      "  7%|▋         | 11919360/170498071 [00:11<00:47, 3305163.18it/s]\u001b[A\n",
      "  7%|▋         | 12394496/170498071 [00:11<00:43, 3611985.69it/s]\u001b[A\n",
      "  7%|▋         | 12771328/170498071 [00:11<00:48, 3284124.09it/s]\u001b[A\n",
      "  8%|▊         | 13197312/170498071 [00:11<00:45, 3494095.04it/s]\u001b[A\n",
      "  8%|▊         | 13606912/170498071 [00:11<00:43, 3647843.83it/s]\u001b[A\n",
      "  8%|▊         | 14082048/170498071 [00:12<00:40, 3853246.10it/s]\u001b[A\n",
      "  9%|▊         | 14622720/170498071 [00:12<00:37, 4203279.20it/s]\u001b[A\n",
      "  9%|▉         | 15065088/170498071 [00:12<00:40, 3818653.93it/s]\u001b[A\n",
      "  9%|▉         | 15589376/170498071 [00:12<00:37, 4157160.51it/s]\u001b[A\n",
      "  9%|▉         | 16039936/170498071 [00:12<00:36, 4255104.01it/s]\u001b[A\n",
      " 10%|▉         | 16523264/170498071 [00:12<00:34, 4408888.48it/s]\u001b[A\n",
      " 10%|▉         | 17047552/170498071 [00:12<00:33, 4569304.89it/s]\u001b[A\n",
      " 10%|█         | 17555456/170498071 [00:12<00:34, 4447606.50it/s]\u001b[A\n",
      " 11%|█         | 18186240/170498071 [00:12<00:31, 4879152.56it/s]\u001b[A\n",
      " 11%|█         | 18694144/170498071 [00:13<00:31, 4840189.95it/s]\u001b[A\n",
      " 11%|█▏        | 19292160/170498071 [00:13<00:29, 5129911.62it/s]\u001b[A\n",
      " 12%|█▏        | 19824640/170498071 [00:13<00:29, 5104784.80it/s]\u001b[A\n",
      " 12%|█▏        | 20348928/170498071 [00:13<00:29, 5055727.79it/s]\u001b[A\n",
      " 12%|█▏        | 21028864/170498071 [00:13<00:29, 4985630.84it/s]\u001b[A\n",
      " 13%|█▎        | 21749760/170498071 [00:13<00:27, 5422968.56it/s]\u001b[A\n",
      " 13%|█▎        | 22306816/170498071 [00:13<00:30, 4820206.52it/s]\u001b[A\n",
      " 14%|█▎        | 23060480/170498071 [00:13<00:28, 5229426.33it/s]\u001b[A\n",
      " 14%|█▍        | 23617536/170498071 [00:13<00:28, 5222197.06it/s]\u001b[A\n",
      " 14%|█▍        | 24207360/170498071 [00:14<00:28, 5125971.60it/s]\u001b[A\n",
      " 15%|█▍        | 24944640/170498071 [00:14<00:25, 5618200.02it/s]\u001b[A\n",
      " 15%|█▍        | 25534464/170498071 [00:14<00:28, 5105019.80it/s]\u001b[A\n",
      " 15%|█▌        | 26173440/170498071 [00:14<00:26, 5426194.99it/s]\u001b[A\n",
      " 16%|█▌        | 26746880/170498071 [00:14<00:27, 5305552.00it/s]\u001b[A\n",
      " 16%|█▌        | 27336704/170498071 [00:14<00:26, 5450429.56it/s]\u001b[A\n",
      " 16%|█▋        | 27926528/170498071 [00:14<00:25, 5553802.97it/s]\u001b[A\n",
      " 17%|█▋        | 28491776/170498071 [00:14<00:26, 5455653.26it/s]\u001b[A\n",
      " 17%|█▋        | 29048832/170498071 [00:14<00:26, 5435630.35it/s]\u001b[A\n",
      " 17%|█▋        | 29597696/170498071 [00:15<00:26, 5323062.68it/s]\u001b[A\n",
      " 18%|█▊        | 30236672/170498071 [00:15<00:25, 5596078.68it/s]\u001b[A\n",
      " 18%|█▊        | 30810112/170498071 [00:15<00:25, 5534343.08it/s]\u001b[A\n",
      " 18%|█▊        | 31449088/170498071 [00:15<00:25, 5375769.94it/s]\u001b[A\n",
      " 19%|█▉        | 32104448/170498071 [00:15<00:24, 5677479.06it/s]\u001b[A\n",
      " 19%|█▉        | 32686080/170498071 [00:15<00:24, 5521158.14it/s]\u001b[A\n",
      " 20%|█▉        | 33251328/170498071 [00:15<00:24, 5509739.67it/s]\u001b[A\n",
      " 20%|█▉        | 33808384/170498071 [00:15<00:24, 5501952.52it/s]\u001b[A\n",
      " 20%|██        | 34414592/170498071 [00:15<00:24, 5609083.42it/s]\u001b[A\n",
      " 21%|██        | 34979840/170498071 [00:16<00:26, 5057770.47it/s]\u001b[A\n",
      " 21%|██        | 35676160/170498071 [00:16<00:24, 5472952.78it/s]\u001b[A\n",
      " 21%|██▏       | 36249600/170498071 [00:16<00:25, 5267622.02it/s]\u001b[A\n",
      " 22%|██▏       | 36970496/170498071 [00:16<00:24, 5491155.72it/s]\u001b[A\n",
      " 22%|██▏       | 37535744/170498071 [00:16<00:24, 5510156.80it/s]\u001b[A\n",
      " 22%|██▏       | 38100992/170498071 [00:16<00:25, 5119332.04it/s]\u001b[A\n",
      " 23%|██▎       | 38690816/170498071 [00:16<00:24, 5329545.91it/s]\u001b[A\n",
      " 23%|██▎       | 39239680/170498071 [00:16<00:24, 5334039.46it/s]\u001b[A\n",
      " 23%|██▎       | 39870464/170498071 [00:16<00:24, 5347825.29it/s]\u001b[A\n",
      " 24%|██▍       | 40558592/170498071 [00:17<00:22, 5666342.06it/s]\u001b[A\n",
      " 24%|██▍       | 41140224/170498071 [00:17<00:23, 5475643.46it/s]\u001b[A\n",
      " 24%|██▍       | 41721856/170498071 [00:17<00:23, 5448432.17it/s]\u001b[A\n",
      " 25%|██▍       | 42278912/170498071 [00:17<00:23, 5398826.70it/s]\u001b[A\n",
      " 25%|██▌       | 42901504/170498071 [00:17<00:22, 5613282.54it/s]\u001b[A\n",
      " 25%|██▌       | 43474944/170498071 [00:17<00:23, 5340142.97it/s]\u001b[A\n",
      " 26%|██▌       | 44048384/170498071 [00:17<00:23, 5442980.93it/s]\u001b[A\n",
      " 26%|██▌       | 44605440/170498071 [00:17<00:25, 4942837.38it/s]\u001b[A\n",
      " 27%|██▋       | 45375488/170498071 [00:17<00:22, 5517537.42it/s]\u001b[A\n",
      " 27%|██▋       | 45965312/170498071 [00:18<00:23, 5219356.36it/s]\u001b[A\n",
      " 27%|██▋       | 46514176/170498071 [00:18<00:25, 4958096.75it/s]\u001b[A\n",
      " 28%|██▊       | 47308800/170498071 [00:18<00:22, 5579718.73it/s]\u001b[A\n",
      " 28%|██▊       | 47906816/170498071 [00:18<00:25, 4897716.45it/s]\u001b[A\n",
      " 28%|██▊       | 48521216/170498071 [00:18<00:23, 5184930.36it/s]\u001b[A\n",
      " 29%|██▉       | 49094656/170498071 [00:18<00:22, 5335227.17it/s]\u001b[A\n",
      " 29%|██▉       | 49659904/170498071 [00:18<00:22, 5379418.93it/s]\u001b[A\n",
      " 29%|██▉       | 50216960/170498071 [00:18<00:23, 5165641.98it/s]\u001b[A\n",
      " 30%|██▉       | 50847744/170498071 [00:18<00:22, 5258694.19it/s]\u001b[A\n",
      " 30%|███       | 51388416/170498071 [00:19<00:22, 5181241.20it/s]\u001b[A\n",
      " 30%|███       | 51994624/170498071 [00:19<00:22, 5271155.21it/s]\u001b[A\n",
      " 31%|███       | 52666368/170498071 [00:19<00:20, 5630655.00it/s]\u001b[A\n",
      " 31%|███       | 53248000/170498071 [00:19<00:21, 5406502.56it/s]\u001b[A\n",
      " 32%|███▏      | 53805056/170498071 [00:19<00:21, 5413190.64it/s]\u001b[A\n",
      " 32%|███▏      | 54370304/170498071 [00:19<00:21, 5408475.56it/s]\u001b[A\n",
      " 32%|███▏      | 54943744/170498071 [00:19<00:21, 5464717.13it/s]\u001b[A\n",
      " 33%|███▎      | 55500800/170498071 [00:19<00:21, 5437669.95it/s]\u001b[A\n",
      " 33%|███▎      | 56123392/170498071 [00:19<00:20, 5603565.64it/s]\u001b[A\n",
      " 33%|███▎      | 56688640/170498071 [00:20<00:20, 5424070.39it/s]\u001b[A\n",
      " 34%|███▎      | 57270272/170498071 [00:20<00:21, 5384702.21it/s]\u001b[A\n",
      " 34%|███▍      | 57892864/170498071 [00:20<00:20, 5517123.12it/s]\u001b[A\n",
      " 34%|███▍      | 58449920/170498071 [00:20<00:21, 5329351.31it/s]\u001b[A\n",
      " 35%|███▍      | 59105280/170498071 [00:20<00:19, 5625479.05it/s]\u001b[A\n",
      " 35%|███▌      | 59678720/170498071 [00:20<00:20, 5493125.84it/s]\u001b[A\n",
      " 35%|███▌      | 60350464/170498071 [00:20<00:19, 5623876.79it/s]\u001b[A\n",
      " 36%|███▌      | 60923904/170498071 [00:20<00:20, 5388863.50it/s]\u001b[A\n",
      " 36%|███▌      | 61497344/170498071 [00:20<00:20, 5423698.72it/s]\u001b[A\n",
      " 36%|███▋      | 62111744/170498071 [00:20<00:19, 5620884.53it/s]\u001b[A\n",
      " 37%|███▋      | 62685184/170498071 [00:21<00:19, 5463357.56it/s]\u001b[A\n",
      " 37%|███▋      | 63315968/170498071 [00:21<00:19, 5620798.33it/s]\u001b[A\n",
      " 37%|███▋      | 63889408/170498071 [00:21<00:20, 5322042.17it/s]\u001b[A\n",
      " 38%|███▊      | 64487424/170498071 [00:21<00:19, 5503656.03it/s]\u001b[A\n",
      " 38%|███▊      | 65044480/170498071 [00:21<00:19, 5361379.37it/s]\u001b[A\n",
      " 39%|███▊      | 65658880/170498071 [00:21<00:18, 5566474.85it/s]\u001b[A\n",
      " 39%|███▉      | 66224128/170498071 [00:21<00:19, 5485282.76it/s]\u001b[A\n",
      " 39%|███▉      | 66854912/170498071 [00:21<00:18, 5608642.66it/s]\u001b[A\n",
      " 40%|███▉      | 67420160/170498071 [00:21<00:18, 5513512.78it/s]\u001b[A\n",
      " 40%|███▉      | 67977216/170498071 [00:22<00:19, 5377541.36it/s]\u001b[A\n",
      " 40%|████      | 68591616/170498071 [00:22<00:18, 5562325.04it/s]\u001b[A\n",
      " 41%|████      | 69156864/170498071 [00:22<00:18, 5448709.76it/s]\u001b[A\n",
      " 41%|████      | 69787648/170498071 [00:22<00:17, 5670704.91it/s]\u001b[A\n",
      " 41%|████▏     | 70361088/170498071 [00:22<00:18, 5469629.90it/s]\u001b[A\n",
      " 42%|████▏     | 70983680/170498071 [00:22<00:18, 5414702.21it/s]\u001b[A\n",
      " 42%|████▏     | 71557120/170498071 [00:22<00:18, 5494171.89it/s]\u001b[A\n",
      " 42%|████▏     | 72130560/170498071 [00:22<00:17, 5474852.18it/s]\u001b[A\n",
      " 43%|████▎     | 72687616/170498071 [00:22<00:18, 5168873.40it/s]\u001b[A\n",
      " 43%|████▎     | 73359360/170498071 [00:23<00:19, 4963462.11it/s]\u001b[A\n",
      " 44%|████▎     | 74194944/170498071 [00:23<00:17, 5611571.42it/s]\u001b[A\n",
      " 44%|████▍     | 74792960/170498071 [00:23<00:18, 5110666.53it/s]\u001b[A\n",
      " 44%|████▍     | 75423744/170498071 [00:23<00:18, 5133122.24it/s]\u001b[A\n",
      " 45%|████▍     | 75997184/170498071 [00:23<00:17, 5283602.92it/s]\u001b[A\n",
      " 45%|████▍     | 76546048/170498071 [00:23<00:17, 5331526.95it/s]\u001b[A\n",
      " 45%|████▌     | 77111296/170498071 [00:23<00:17, 5417287.95it/s]\u001b[A\n",
      " 46%|████▌     | 77668352/170498071 [00:23<00:17, 5415206.48it/s]\u001b[A\n",
      " 46%|████▌     | 78225408/170498071 [00:23<00:17, 5329105.42it/s]\u001b[A\n",
      " 46%|████▌     | 78848000/170498071 [00:24<00:16, 5431443.11it/s]\u001b[A\n",
      " 47%|████▋     | 79421440/170498071 [00:24<00:16, 5516478.43it/s]\u001b[A\n",
      " 47%|████▋     | 79978496/170498071 [00:24<00:16, 5372454.02it/s]\u001b[A\n",
      " 47%|████▋     | 80650240/170498071 [00:24<00:15, 5684279.46it/s]\u001b[A\n",
      " 48%|████▊     | 81231872/170498071 [00:24<00:16, 5491073.22it/s]\u001b[A\n",
      " 48%|████▊     | 81797120/170498071 [00:24<00:16, 5414094.93it/s]\u001b[A\n",
      " 48%|████▊     | 82370560/170498071 [00:24<00:16, 5419084.55it/s]\u001b[A\n",
      " 49%|████▊     | 82919424/170498071 [00:24<00:16, 5379683.86it/s]\u001b[A\n",
      " 49%|████▉     | 83533824/170498071 [00:24<00:15, 5586803.39it/s]\u001b[A\n",
      " 49%|████▉     | 84099072/170498071 [00:25<00:16, 5399832.02it/s]\u001b[A\n",
      " 50%|████▉     | 84746240/170498071 [00:25<00:15, 5670077.57it/s]\u001b[A\n",
      " 50%|█████     | 85319680/170498071 [00:25<00:15, 5335304.54it/s]\u001b[A\n",
      " 50%|█████     | 85942272/170498071 [00:25<00:15, 5482175.03it/s]\u001b[A\n",
      " 51%|█████     | 86532096/170498071 [00:25<00:15, 5581963.67it/s]\u001b[A\n",
      " 51%|█████     | 87097344/170498071 [00:25<00:15, 5511013.16it/s]\u001b[A\n",
      " 51%|█████▏    | 87711744/170498071 [00:25<00:14, 5649739.03it/s]\u001b[A\n",
      " 52%|█████▏    | 88285184/170498071 [00:25<00:14, 5538273.29it/s]\u001b[A\n",
      " 52%|█████▏    | 88940544/170498071 [00:25<00:14, 5581331.23it/s]\u001b[A\n",
      " 52%|█████▏    | 89505792/170498071 [00:26<00:14, 5400275.52it/s]\u001b[A\n",
      " 53%|█████▎    | 90103808/170498071 [00:26<00:14, 5561565.91it/s]\u001b[A\n",
      " 53%|█████▎    | 90677248/170498071 [00:26<00:14, 5521873.50it/s]\u001b[A\n",
      " 54%|█████▎    | 91234304/170498071 [00:26<00:14, 5479999.32it/s]\u001b[A\n",
      " 54%|█████▍    | 91889664/170498071 [00:26<00:13, 5637578.02it/s]\u001b[A\n",
      " 54%|█████▍    | 92463104/170498071 [00:26<00:14, 5307272.65it/s]\u001b[A\n",
      " 55%|█████▍    | 93118464/170498071 [00:26<00:13, 5602934.74it/s]\u001b[A\n",
      " 55%|█████▍    | 93691904/170498071 [00:26<00:14, 5303747.26it/s]\u001b[A\n",
      " 55%|█████▌    | 94330880/170498071 [00:26<00:13, 5540343.87it/s]\u001b[A\n",
      " 56%|█████▌    | 94896128/170498071 [00:26<00:13, 5474634.53it/s]\u001b[A\n",
      " 56%|█████▌    | 95510528/170498071 [00:27<00:13, 5576594.84it/s]\u001b[A\n",
      " 56%|█████▋    | 96075776/170498071 [00:27<00:14, 5206794.48it/s]\u001b[A\n",
      " 57%|█████▋    | 96755712/170498071 [00:27<00:13, 5588200.27it/s]\u001b[A\n",
      " 57%|█████▋    | 97329152/170498071 [00:27<00:13, 5404830.50it/s]\u001b[A\n",
      " 57%|█████▋    | 97902592/170498071 [00:27<00:13, 5467781.43it/s]\u001b[A\n",
      " 58%|█████▊    | 98492416/170498071 [00:27<00:12, 5579215.00it/s]\u001b[A\n",
      " 58%|█████▊    | 99057664/170498071 [00:27<00:13, 5125037.87it/s]\u001b[A\n",
      " 58%|█████▊    | 99581952/170498071 [00:27<00:14, 5018762.24it/s]\u001b[A\n",
      " 59%|█████▉    | 100245504/170498071 [00:27<00:13, 5391367.66it/s]\u001b[A\n",
      " 59%|█████▉    | 100802560/170498071 [00:28<00:13, 5252340.86it/s]\u001b[A\n",
      " 60%|█████▉    | 101457920/170498071 [00:28<00:12, 5457672.02it/s]\u001b[A\n",
      " 60%|█████▉    | 102014976/170498071 [00:28<00:12, 5450968.28it/s]\u001b[A\n",
      " 60%|██████    | 102572032/170498071 [00:28<00:12, 5283021.99it/s]\u001b[A\n",
      " 60%|██████    | 103112704/170498071 [00:28<00:12, 5224176.85it/s]\u001b[A\n",
      " 61%|██████    | 103735296/170498071 [00:28<00:12, 5403922.02it/s]\u001b[A\n",
      " 61%|██████    | 104284160/170498071 [00:28<00:12, 5412581.52it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 104865792/170498071 [00:28<00:12, 5262283.82it/s]\u001b[A\n",
      " 62%|██████▏   | 105553920/170498071 [00:28<00:11, 5657336.87it/s]\u001b[A\n",
      " 62%|██████▏   | 106135552/170498071 [00:29<00:12, 5353884.65it/s]\u001b[A\n",
      " 63%|██████▎   | 106700800/170498071 [00:29<00:11, 5423421.96it/s]\u001b[A\n",
      " 63%|██████▎   | 107257856/170498071 [00:29<00:11, 5435088.95it/s]\u001b[A\n",
      " 63%|██████▎   | 107814912/170498071 [00:29<00:11, 5468134.81it/s]\u001b[A\n",
      " 64%|██████▎   | 108371968/170498071 [00:29<00:12, 5100406.75it/s]\u001b[A\n",
      " 64%|██████▍   | 109043712/170498071 [00:29<00:11, 5431347.14it/s]\u001b[A\n",
      " 64%|██████▍   | 109600768/170498071 [00:29<00:11, 5180866.03it/s]\u001b[A\n",
      " 65%|██████▍   | 110256128/170498071 [00:29<00:10, 5525469.07it/s]\u001b[A\n",
      " 65%|██████▌   | 110829568/170498071 [00:29<00:11, 5120263.94it/s]\u001b[A\n",
      " 65%|██████▌   | 111484928/170498071 [00:30<00:10, 5436835.58it/s]\u001b[A\n",
      " 66%|██████▌   | 112050176/170498071 [00:30<00:10, 5390378.53it/s]\u001b[A\n",
      " 66%|██████▌   | 112607232/170498071 [00:30<00:10, 5420758.84it/s]\u001b[A\n",
      " 66%|██████▋   | 113172480/170498071 [00:30<00:10, 5485586.48it/s]\u001b[A\n",
      " 67%|██████▋   | 113729536/170498071 [00:30<00:10, 5500892.86it/s]\u001b[A\n",
      " 67%|██████▋   | 114286592/170498071 [00:30<00:10, 5357515.81it/s]\u001b[A\n",
      " 67%|██████▋   | 114925568/170498071 [00:30<00:09, 5566172.49it/s]\u001b[A\n",
      " 68%|██████▊   | 115490816/170498071 [00:30<00:10, 5384772.23it/s]\u001b[A\n",
      " 68%|██████▊   | 116056064/170498071 [00:30<00:10, 5421174.26it/s]\u001b[A\n",
      " 68%|██████▊   | 116662272/170498071 [00:30<00:09, 5588931.31it/s]\u001b[A\n",
      " 69%|██████▉   | 117227520/170498071 [00:31<00:09, 5408213.08it/s]\u001b[A\n",
      " 69%|██████▉   | 117858304/170498071 [00:31<00:09, 5443179.94it/s]\u001b[A\n",
      " 69%|██████▉   | 118431744/170498071 [00:31<00:09, 5526639.20it/s]\u001b[A\n",
      " 70%|██████▉   | 118988800/170498071 [00:31<00:09, 5304160.20it/s]\u001b[A\n",
      " 70%|███████   | 119693312/170498071 [00:31<00:09, 5595132.46it/s]\u001b[A\n",
      " 71%|███████   | 120266752/170498071 [00:31<00:09, 5464067.31it/s]\u001b[A\n",
      " 71%|███████   | 120905728/170498071 [00:31<00:08, 5709191.50it/s]\u001b[A\n",
      " 71%|███████▏  | 121487360/170498071 [00:31<00:09, 5144426.42it/s]\u001b[A\n",
      " 72%|███████▏  | 122118144/170498071 [00:31<00:08, 5433715.26it/s]\u001b[A\n",
      " 72%|███████▏  | 122683392/170498071 [00:32<00:09, 5276729.02it/s]\u001b[A\n",
      " 72%|███████▏  | 123314176/170498071 [00:32<00:08, 5454964.74it/s]\u001b[A\n",
      " 73%|███████▎  | 123871232/170498071 [00:32<00:08, 5234158.81it/s]\u001b[A\n",
      " 73%|███████▎  | 124510208/170498071 [00:32<00:08, 5526774.96it/s]\u001b[A\n",
      " 73%|███████▎  | 125075456/170498071 [00:32<00:08, 5127365.74it/s]\u001b[A\n",
      " 74%|███████▍  | 125788160/170498071 [00:32<00:08, 5574983.80it/s]\u001b[A\n",
      " 74%|███████▍  | 126369792/170498071 [00:32<00:08, 5347026.55it/s]\u001b[A\n",
      " 74%|███████▍  | 126926848/170498071 [00:32<00:08, 5000940.76it/s]\u001b[A\n",
      " 75%|███████▍  | 127590400/170498071 [00:33<00:07, 5384158.08it/s]\u001b[A\n",
      " 75%|███████▌  | 128155648/170498071 [00:33<00:08, 5006025.83it/s]\u001b[A\n",
      " 76%|███████▌  | 128901120/170498071 [00:33<00:07, 5390319.93it/s]\u001b[A\n",
      " 76%|███████▌  | 129466368/170498071 [00:33<00:07, 5369055.87it/s]\u001b[A\n",
      " 76%|███████▋  | 130031616/170498071 [00:33<00:07, 5418928.25it/s]\u001b[A\n",
      " 77%|███████▋  | 130605056/170498071 [00:33<00:07, 5504958.60it/s]\u001b[A\n",
      " 77%|███████▋  | 131170304/170498071 [00:33<00:07, 5302562.99it/s]\u001b[A\n",
      " 77%|███████▋  | 131710976/170498071 [00:33<00:07, 5167288.58it/s]\u001b[A\n",
      " 78%|███████▊  | 132308992/170498071 [00:33<00:07, 5344831.32it/s]\u001b[A\n",
      " 78%|███████▊  | 132882432/170498071 [00:33<00:06, 5443737.99it/s]\u001b[A\n",
      " 78%|███████▊  | 133455872/170498071 [00:34<00:06, 5508014.23it/s]\u001b[A\n",
      " 79%|███████▊  | 134012928/170498071 [00:34<00:06, 5456646.85it/s]\u001b[A\n",
      " 79%|███████▉  | 134586368/170498071 [00:34<00:06, 5520046.95it/s]\u001b[A\n",
      " 79%|███████▉  | 135143424/170498071 [00:34<00:06, 5511154.77it/s]\u001b[A\n",
      " 80%|███████▉  | 135749632/170498071 [00:34<00:06, 5651740.40it/s]\u001b[A\n",
      " 80%|███████▉  | 136323072/170498071 [00:34<00:06, 5305946.77it/s]\u001b[A\n",
      " 80%|████████  | 136945664/170498071 [00:34<00:06, 5551031.17it/s]\u001b[A\n",
      " 81%|████████  | 137510912/170498071 [00:34<00:06, 5362072.81it/s]\u001b[A\n",
      " 81%|████████  | 138174464/170498071 [00:34<00:05, 5644464.58it/s]\u001b[A\n",
      " 81%|████████▏ | 138747904/170498071 [00:35<00:06, 5155578.41it/s]\u001b[A\n",
      " 82%|████████▏ | 139403264/170498071 [00:35<00:05, 5493572.07it/s]\u001b[A\n",
      " 82%|████████▏ | 139976704/170498071 [00:35<00:05, 5446427.56it/s]\u001b[A\n",
      " 82%|████████▏ | 140582912/170498071 [00:35<00:05, 5596050.61it/s]\u001b[A\n",
      " 83%|████████▎ | 141172736/170498071 [00:35<00:05, 5665457.26it/s]\u001b[A\n",
      " 83%|████████▎ | 141746176/170498071 [00:35<00:05, 5525303.33it/s]\u001b[A\n",
      " 83%|████████▎ | 142311424/170498071 [00:35<00:05, 5529256.11it/s]\u001b[A\n",
      " 84%|████████▍ | 142868480/170498071 [00:35<00:04, 5540279.56it/s]\u001b[A\n",
      " 84%|████████▍ | 143425536/170498071 [00:35<00:04, 5430974.90it/s]\u001b[A\n",
      " 84%|████████▍ | 143974400/170498071 [00:36<00:04, 5413041.21it/s]\u001b[A\n",
      " 85%|████████▍ | 144613376/170498071 [00:36<00:04, 5621580.79it/s]\u001b[A\n",
      " 85%|████████▌ | 145186816/170498071 [00:36<00:04, 5440625.58it/s]\u001b[A\n",
      " 86%|████████▌ | 145776640/170498071 [00:36<00:04, 5208544.49it/s]\u001b[A\n",
      " 86%|████████▌ | 146464768/170498071 [00:36<00:04, 5608915.12it/s]\u001b[A\n",
      " 86%|████████▌ | 147038208/170498071 [00:36<00:04, 5328609.61it/s]\u001b[A\n",
      " 87%|████████▋ | 147628032/170498071 [00:36<00:04, 5429468.17it/s]\u001b[A\n",
      " 87%|████████▋ | 148201472/170498071 [00:36<00:04, 5507789.09it/s]\u001b[A\n",
      " 87%|████████▋ | 148791296/170498071 [00:36<00:03, 5555365.12it/s]\u001b[A\n",
      " 88%|████████▊ | 149356544/170498071 [00:37<00:04, 5166366.71it/s]\u001b[A\n",
      " 88%|████████▊ | 149987328/170498071 [00:37<00:03, 5446269.06it/s]\u001b[A\n",
      " 88%|████████▊ | 150544384/170498071 [00:37<00:03, 5373191.60it/s]\u001b[A\n",
      " 89%|████████▊ | 151134208/170498071 [00:37<00:03, 5489148.64it/s]\u001b[A\n",
      " 89%|████████▉ | 151707648/170498071 [00:37<00:03, 5514468.56it/s]\u001b[A\n",
      " 89%|████████▉ | 152264704/170498071 [00:37<00:03, 5487162.23it/s]\u001b[A\n",
      " 90%|████████▉ | 152821760/170498071 [00:37<00:03, 5172182.35it/s]\u001b[A\n",
      " 90%|████████▉ | 153411584/170498071 [00:37<00:03, 5328088.55it/s]\u001b[A\n",
      " 90%|█████████ | 153985024/170498071 [00:37<00:03, 5425140.99it/s]\u001b[A\n",
      " 91%|█████████ | 154558464/170498071 [00:37<00:02, 5461030.29it/s]\u001b[A\n",
      " 91%|█████████ | 155131904/170498071 [00:38<00:02, 5533077.96it/s]\u001b[A\n",
      " 91%|█████████▏| 155688960/170498071 [00:38<00:02, 5513304.31it/s]\u001b[A\n",
      " 92%|█████████▏| 156278784/170498071 [00:38<00:02, 5535717.07it/s]\u001b[A\n",
      " 92%|█████████▏| 156884992/170498071 [00:38<00:02, 5622564.84it/s]\u001b[A\n",
      " 92%|█████████▏| 157450240/170498071 [00:38<00:02, 5439185.73it/s]\u001b[A\n",
      " 93%|█████████▎| 158048256/170498071 [00:38<00:02, 5528797.17it/s]\u001b[A\n",
      " 93%|█████████▎| 158654464/170498071 [00:38<00:02, 5654399.13it/s]\u001b[A\n",
      " 93%|█████████▎| 159227904/170498071 [00:38<00:02, 5523311.58it/s]\u001b[A\n",
      " 94%|█████████▎| 159784960/170498071 [00:38<00:01, 5531162.20it/s]\u001b[A\n",
      " 94%|█████████▍| 160391168/170498071 [00:38<00:01, 5679036.99it/s]\u001b[A\n",
      " 94%|█████████▍| 160964608/170498071 [00:39<00:01, 5168806.68it/s]\u001b[A\n",
      " 95%|█████████▍| 161636352/170498071 [00:39<00:01, 5512512.74it/s]\u001b[A\n",
      " 95%|█████████▌| 162201600/170498071 [00:39<00:01, 5424281.48it/s]\u001b[A\n",
      " 95%|█████████▌| 162758656/170498071 [00:39<00:01, 5389501.43it/s]\u001b[A\n",
      " 96%|█████████▌| 163307520/170498071 [00:39<00:01, 4912204.02it/s]\u001b[A\n",
      " 96%|█████████▌| 164093952/170498071 [00:39<00:01, 5372960.46it/s]\u001b[A\n",
      " 97%|█████████▋| 164659200/170498071 [00:39<00:01, 5388286.68it/s]\u001b[A\n",
      " 97%|█████████▋| 165273600/170498071 [00:39<00:00, 5571036.49it/s]\u001b[A\n",
      " 97%|█████████▋| 165847040/170498071 [00:40<00:00, 5322588.40it/s]\u001b[A\n",
      " 98%|█████████▊| 166395904/170498071 [00:40<00:00, 5285143.77it/s]\u001b[A\n",
      " 98%|█████████▊| 167010304/170498071 [00:40<00:00, 5508385.49it/s]\u001b[A\n",
      " 98%|█████████▊| 167575552/170498071 [00:40<00:00, 5396236.15it/s]\u001b[A\n",
      " 99%|█████████▊| 168255488/170498071 [00:40<00:00, 5692240.02it/s]\u001b[A\n",
      " 99%|█████████▉| 168837120/170498071 [00:40<00:00, 5467420.75it/s]\u001b[A\n",
      " 99%|█████████▉| 169394176/170498071 [00:40<00:00, 5176477.99it/s]\u001b[A\n",
      "100%|█████████▉| 170024960/170498071 [00:40<00:00, 5383276.47it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "170500096it [00:55, 5383276.47it/s]                               \u001b[A"
     ]
    }
   ],
   "source": [
    "#画像の変形処理\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "#CIFAR-10のTensorに変形前のtrainsetのロード\n",
    "rawtrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True)\n",
    "\n",
    "#CIFAR-10のtrain, testsetのロード\n",
    "#変形はtransformを適用\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "#DataLoaderの適用->これによりバッチの割り当て・シャッフルをまとめて行うことができる\n",
    "#batch_sizeでバッチサイズを指定\n",
    "#num_workersでいくつのコアでデータをロードするか指定(デフォルトはメインのみ)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mnistのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=12, shuffle=True)#, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=12, shuffle=True)#, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd(自動微分)\n",
    "\n",
    "Tensorには**requires_grad**という属性があり、これをTrueにすることで自動微分を行うフラグが有効になります。<br>\n",
    "ニューラルネットワークを扱う場合、パラメータやデータはすべてこのフラグが有効になっています。<br>\n",
    "PyTorch 0.3以前は自動微分を使用するためにはVariableというクラスでTensorをラップする必要がありましたが、0.4からはTensorとVariableが統合されました。<br>\n",
    "requires_gradが有効なTensorに対して様々な演算を積み重ねていくことで計算グラフが構築され、backwardメソッドを呼ぶと、その情報から自動的に微分を計算することができます。以下では、\n",
    "$$y_i=ax_i\\\\\n",
    "L=\\Sigma_iy_i$$\n",
    "で計算されるLを$a_k$についてについて微分をしてみます。<br>\n",
    "このシンプルな例では解析的に解けて、\n",
    "$$\\frac{\\partial L}{\\partial a_k}=\\Sigma_ix_{ik}$$\n",
    "となりますが、これを自動微分で求めてみます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_fn  <MvBackward object at 0x119937908>\n",
      "aで微分\n",
      " tensor([12., 15., 18.])\n",
      "tensor([1, 1, 1], dtype=torch.uint8)\n",
      "xで微分\n",
      " tensor([[1., 2., 3.],\n",
      "        [1., 2., 3.],\n",
      "        [1., 2., 3.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3.],[4,5,6.],[7,8,9.]],requires_grad=True)\n",
    "# 微分の変数として扱う場合はrequires_gradフラグをTrueにする\n",
    "a = torch.tensor([1, 2, 3.], requires_grad=True)\n",
    "\n",
    "# 計算をすることで自動的に計算グラフが構築されていく\n",
    "y = torch.mv(x, a)#行列xとベクトルyの内積です.\n",
    "L = y.sum()\n",
    "print(\"grad_fn \",y.grad_fn)\n",
    "\n",
    "# 微分を実行する\n",
    "L.backward()\n",
    "print(\"aで微分\\n\",a.grad) #xaという演算はaというベクトルにaの各行をかけたものの和となるので、勾配はxの各行の和のベクトルとなる。\n",
    "# 解析解と比較. tensorではbooleanは0,1で表現されるらしい.\n",
    "print(a.grad == x.sum(0))\n",
    "a.detach()\n",
    "print(\"xで微分\\n\",x.grad) #1,2,3というベクトルがaの3つのベクトルにかけられていると解釈できる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 関数を用いた演算の勾配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最初のloss :  tensor(2.0317, grad_fn=<MseLossBackward>)\n",
      "一回学習後  :  tensor(2.0317, grad_fn=<MseLossBackward>)\n",
      "沢山学習後  :  tensor(1.7883, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10,3)\n",
    "y = torch.randn(10,2)\n",
    "linear = nn.Linear(3,2)\n",
    "\n",
    "# Build loss function and optimizer.\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(linear.parameters(),lr=0.01)\n",
    "pred = linear(x)\n",
    "loss = criterion(pred,y)\n",
    "print(\"最初のloss : \",loss)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "# You can also perform gradient descent at the low level.\n",
    "# linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n",
    "# linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n",
    "pred= linear(x)\n",
    "print(\"一回学習後  : \",loss)\n",
    "\n",
    "#以下、もっと学習してみる・・・(ただし、データがランダムなのでそんなにロスは落ちません)\n",
    "for i in range(1000):\n",
    "    loss = criterion(pred,y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    pred= linear(x)\n",
    "loss = criterion(pred,y)\n",
    "print(\"沢山学習後  : \",loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 線型回帰モデル\n",
    "### 直感的な実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#真の分布\n",
    "w_true = torch.Tensor([1,2,3])\n",
    "\n",
    "#Xのデータの準備。切片を回帰係数に含めるため、Xの最初の次元に1を追加しておく。\n",
    "X = torch.cat([torch.ones(100,1), torch.randn(100,2)], 1)\n",
    "#真の係数と各Xとの内積を行列とベクトルの積でまとめて計算\n",
    "y = torch.mv(X,w_true) + torch.randn(100)*0.5\n",
    "\n",
    "#勾配降下で最適化するためのパラメータのTensorを乱数で初期化して作成\n",
    "w = torch.randn(3, requires_grad = True)\n",
    "\n",
    "#学習率\n",
    "gamma = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#損失関数のログ\n",
    "losses = []\n",
    "\n",
    "#100回イテレーションを回す\n",
    "for epoc in range(100):\n",
    "    #前回のepochのbackwardメソッドで計算された勾配の値を削除\n",
    "    w.grad = None\n",
    "    \n",
    "    #線型モデルでyの予測値を計算\n",
    "    y_pred = torch.mv(X, w)\n",
    "    \n",
    "    #MSE lossとwによる微分を計算\n",
    "    loss = torch.mean((y - y_pred)**2)\n",
    "    loss.backward()\n",
    "    \n",
    "    #勾配を更新する\n",
    "    #wをそのまま代入して更新すると異なるTensorになって計算グラフが破壊されてしまうのでdataだけを更新する。\n",
    "    w.data = w.data - gamma * w.grad.data\n",
    "    \n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11a2612b0>]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#lossの変化をプロット\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0078, 2.0618, 3.0018], requires_grad=True)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#最終的に推定されたwの値は以下の通り\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.optimの利用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x11a23c5c8>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear層を作成。今回は切片項は回帰係数に含めるので、入力の次元を3とし、bias(切片）をFalseにする\n",
    "net = nn.Linear(in_features=3,out_features=1,bias=False)\n",
    "#SGDのオブティマイザーに上で定義したネットワークのパラメータを渡して初期化\n",
    "optimizer = optim.SGD(net.parameters(),lr=0.1)\n",
    "#MSELossクラス\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#損失関数のログ\n",
    "losses = []\n",
    "\n",
    "#100回イテレーションを回す\n",
    "for epoc in range(100):\n",
    "    #前回のbackwardメソッドで計算された勾配の値を削除\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #線型モデルでyの予測値を計算\n",
    "    y_pred = net(X)\n",
    "    \n",
    "    #MSE_lossを計算\n",
    "    #y_predは(n,1)のようなshapeを持っているので(n,)に直す必要がある\n",
    "    loss = loss_fn(y_pred.view_as(y), y)\n",
    "    \n",
    "    #lossのwによる微分を計算\n",
    "    loss.backward()\n",
    "    \n",
    "    #勾配を更新する\n",
    "    optimizer.step()\n",
    "    \n",
    "    #収束確認のためにlossを記録\n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13.152225494384766,\n",
       " 7.9776082038879395,\n",
       " 4.887302875518799,\n",
       " 3.039928913116455,\n",
       " 1.9343810081481934,\n",
       " 1.2719944715499878,\n",
       " 0.8746193647384644,\n",
       " 0.6358981728553772,\n",
       " 0.4922723174095154,\n",
       " 0.40572088956832886,\n",
       " 0.35347306728363037,\n",
       " 0.3218744695186615,\n",
       " 0.30272626876831055,\n",
       " 0.2910984754562378,\n",
       " 0.28402167558670044,\n",
       " 0.27970457077026367,\n",
       " 0.2770645022392273,\n",
       " 0.2754458487033844,\n",
       " 0.27445077896118164,\n",
       " 0.2738374173641205,\n",
       " 0.2734582722187042,\n",
       " 0.2732231914997101,\n",
       " 0.2730770409107208,\n",
       " 0.27298590540885925,\n",
       " 0.2729288637638092,\n",
       " 0.27289310097694397,\n",
       " 0.27287057042121887,\n",
       " 0.27285635471343994,\n",
       " 0.27284741401672363,\n",
       " 0.2728416621685028,\n",
       " 0.2728380858898163,\n",
       " 0.27283579111099243,\n",
       " 0.27283427119255066,\n",
       " 0.27283337712287903,\n",
       " 0.2728327214717865,\n",
       " 0.27283230423927307,\n",
       " 0.27283206582069397,\n",
       " 0.27283191680908203,\n",
       " 0.2728317975997925,\n",
       " 0.2728317677974701,\n",
       " 0.27283167839050293,\n",
       " 0.27283164858818054,\n",
       " 0.27283161878585815,\n",
       " 0.27283164858818054,\n",
       " 0.27283158898353577,\n",
       " 0.27283161878585815,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283161878585815,\n",
       " 0.27283161878585815,\n",
       " 0.27283158898353577,\n",
       " 0.27283161878585815,\n",
       " 0.27283161878585815,\n",
       " 0.27283161878585815,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283161878585815,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283161878585815,\n",
       " 0.27283161878585815,\n",
       " 0.27283161878585815,\n",
       " 0.27283158898353577,\n",
       " 0.27283161878585815,\n",
       " 0.27283161878585815,\n",
       " 0.27283161878585815,\n",
       " 0.27283161878585815,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577,\n",
       " 0.27283158898353577]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11a338cc0>]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAFGxJREFUeJzt3WuMXGd9x/Hvf2Zndya2dx3H60suqtPETUxoU9BSAUG0SoKaQgSoqlRQLymNlDe9hKoSAvGC9k2hLaJAL1RuEkJbmlYNFFJEobmA0gtN2UBKk9iQEAJx6stCsJ3EXnvX+++LmY3X651de2d2x+fM9yOtdubs2Tn/o5P89vFznuc8kZlIkoqv0usCJEndYaBLUkkY6JJUEga6JJWEgS5JJWGgS1JJGOiSVBIGuiSVhIEuSSUxsJoH27hxY27btm01DylJhffwww9/PzNHl9pvVQN927ZtjI+Pr+YhJanwIuK7Z7KfXS6SVBIGuiSVhIEuSSVhoEtSSRjoklQSBroklYSBLkklUYhAv3/Xfv7iy0/2ugxJOqcVItAf/NYEOx98qtdlSNI5rRCBXh+scvT4iV6XIUnntEIEeqNW5dj0DDMz2etSJOmcVZhAB5ictpUuSe0UI9AHW4E+NdPjSiTp3FWIQK+3WuhHp2yhS1I7hQj02S4Xb4xKUnuFCPTZFvqkLXRJaqsQgd6wy0WSllSMQB9slmmXiyS1V4hA96aoJC2tEIHesA9dkpZUjEAfdJSLJC2lGIFul4skLWnJQI+IOyLiQEQ8OmfbH0fE7oj4RkT8U0SsX8ki7UOXpKWdSQv9TuCGedvuBV6emT8BfAt4T5frOsXQQIUImLTLRZLaWjLQM/NB4Ll52/41M6dbb/8LuHgFantJRNCoVZmc9lkuktRON/rQfx34ly58zqLqNZ+JLkmL6SjQI+K9wDTwyUX2uSUixiNifGJiYtnHatSq9qFL0iKWHegR8WvAjcAvZWbblScyc2dmjmXm2Ojo6HIPR71WMdAlaREDy/mliLgBeBfw05l5pLslLawxWPWmqCQt4kyGLd4FfAW4IiL2RMTNwJ8B64B7I+KRiPjLFa7TLhdJWsKSLfTMfPsCm29fgVoWVa9VeeHY9NI7SlKfKsRMUWi10O1ykaS2ihPog1UfziVJiyhOoNuHLkmLKkygO7FIkhZXqECfnHLqvyS1U5hAb9SqHD8xw4mZtnOYJKmvFSfQW+uKemNUkhZWnED3meiStKjCBPpLi1x4Y1SSFlSYQJ9dV9QuF0laWHEC3S4XSVpU8QLdLhdJWlBhAr0+aAtdkhZTmECfbaHbhy5JCytcoNtCl6SFFSbQTw5bdPq/JC2kMIFuC12SFleYQK879V+SFlWYQB+sVqiEgS5J7RQm0CPCZegkaRGFCXRoTv+3D12SFlaoQK+7DJ0ktbVkoEfEHRFxICIenbNtQ0TcGxFPtL6fv7JlNjVqLhQtSe2cSQv9TuCGedveDdyfmduB+1vvV1xj0D50SWpnyUDPzAeB5+ZtfgvwidbrTwBv7XJdC7LLRZLaW24f+ubM3Nt6vQ/Y3KV6FtUMdGeKStJCOr4pmpkJtF25OSJuiYjxiBifmJjo6FiNWoVJu1wkaUHLDfT9EbEVoPX9QLsdM3NnZo5l5tjo6OgyD9fUsMtFktpabqDfA9zUen0T8NnulLM4x6FLUntnMmzxLuArwBURsScibgY+ALwhIp4Arm+9X3H1WtUuF0lqY2CpHTLz7W1+dF2Xa1lSo1ZlctpAl6SFFGqmaKNWZepEMnXCkS6SNF+xAn3QZegkqZ1CBXrdRS4kqa1CBfpLC0W7DJ0knaZQgW4LXZLaK1SgN1rL0BnoknS6QgX6Sy10x6JL0mkKFegv9aHbQpek0xQr0AftQ5ekdooV6Ha5SFJbxQx0W+iSdJpCBXrdmaKS1FahAt2bopLUXqECvVatMFAJu1wkaQGFCnRorVrk1H9JOk3hAn3IZegkaUGFC/TGYMU+dElaQPECvVZ1HLokLaCYgW4LXZJOU7hArxvokrSgwgV6Y7BqH7okLaB4gW4fuiQtqKNAj4jfiYjHIuLRiLgrIurdKqwd+9AlaWHLDvSIuAj4bWAsM18OVIG3dauwduqDVSannFgkSfN12uUyADQiYgA4D/i/zktaXKNmH7okLWTZgZ6ZzwIfBL4H7AUOZea/zt8vIm6JiPGIGJ+YmFh+pS2NWpUjx6fJzI4/S5LKpJMul/OBtwCXAhcCayLil+fvl5k7M3MsM8dGR0eXX2nL2voAMwlHvDEqSafopMvleuA7mTmRmVPAp4HXdqes9kYaNQAOHZ1a6UNJUqF0EujfA14dEedFRADXAbu6U1Z7s4F+eNJAl6S5OulDfwi4G/ga8L+tz9rZpbraGq63WuhHDHRJmmugk1/OzPcB7+tSLWfELhdJWljhZooON5p/gw5PTve4Ekk6txQu0G2hS9LCChfo61p96IcNdEk6ReECvVoJ1g0N2EKXpHkKF+gAw42aLXRJmqe4ge44dEk6RSEDfaRhl4skzVfQQK9x+KjDFiVprkIG+nC9ZgtdkuYpZKCP2IcuSacpZKAPN2ocOX6CqROuXCRJswoZ6M4WlaTTFTrQHYsuSScVMtBnH9BlC12STipkoJ9c5MKhi5I0q9CBbgtdkk4qZKC/tGqRgS5JLylmoHtTVJJOU8hAr9eqDA5UDHRJmqOQgQ7OFpWk+Qod6PahS9JJHQV6RKyPiLsjYndE7IqI13SrsKUM1wd84qIkzTHQ4e9/BPhCZv5CRAwC53WhpjMy0qjx/ReOr9bhJOmct+wWekSMAK8HbgfIzOOZebBbhS1l2C4XSTpFJ10ulwITwMcj4usRcVtErOlSXUvypqgknaqTQB8AXgl8LDNfAbwIvHv+ThFxS0SMR8T4xMREB4c71UhroeiZmezaZ0pSkXUS6HuAPZn5UOv93TQD/hSZuTMzxzJzbHR0tIPDnWq4XmMm4cXj3hiVJOgg0DNzH/BMRFzR2nQd8HhXqjoDPs9Fkk7V6SiX3wI+2Rrh8hTwjs5LOjOzj9A9fHQazl+to0rSuaujQM/MR4CxLtVyVoZtoUvSKQo9UxQMdEmaVdhAn32ErkMXJampsIE+cp6P0JWkuQob6GsHB4gw0CVpVmEDvVIJhutO/5ekWYUNdPARupI0V6EDfbgxwOFJZ4pKEhQ80G2hS9JJhQ704XrNm6KS1FLoQLeFLkknFT7QnVgkSU2FDvThRo3JqRmOTZ/odSmS1HOFDvQNawYBXFtUkih4oG8ZqQOw79BkjyuRpN4rdqAPNwN9/2EDXZIKHehbWy30vbbQJanYgT7SqFGvVdh36GivS5Gknit0oEcEW0cattAliYIHOsDm4SH70CWJEgS6LXRJaip8oG8ZqbP/8CQzM9nrUiSppwof6FtH6kydSH7wopOLJPW3jgM9IqoR8fWI+Fw3Cjpbmx2LLklAd1rotwK7uvA5y+JYdElq6ijQI+Ji4E3Abd0p5+ydnP7vWHRJ/a3TFvqHgXcBM12oZVk2rhlioBK20CX1vWUHekTcCBzIzIeX2O+WiBiPiPGJiYnlHq6tSiXYPFxnn33okvpcJy30a4A3R8TTwN8D10bE387fKTN3ZuZYZo6Njo52cLj2tozUfeKipL637EDPzPdk5sWZuQ14G/BAZv5y1yo7Cwa6JJVgHDrA1uE6ew9NkunkIkn9qyuBnplfzswbu/FZy7FlpM7RqRMcnpzuVQmS1HOlaKG7cpEklSTQT04uciy6pP5VikDfMtIAbKFL6m+lCPRN64aIwLHokvpaKQK9Vq2wce2QLXRJfa0UgQ7NfnSn/0vqZ6UJ9C3DTi6S1N/KE+gjdUe5SOprpQr0w5PTHDnu5CJJ/ak0gX7R+ubQxT0/tJUuqT+VJtC3b1oHwO59z/e4EknqjdIE+uWb1jJQCXbvPdzrUiSpJ0oT6IMDFS7ftNYWuqS+VZpAB7hyyzp22UKX1KdKFeg7tg6z99AkB48c73UpkrTqShXoV24dBrwxKqk/lSrQd2xpjnSx20VSPypVoI+uG+KCNYPs3msLXVL/KVWgRwRXbl3Hrn220CX1n1IFOsCOLcN8c9/znJhxwWhJ/aV0gX7l1mGOTc/w9A9e7HUpkrSqyhfo3hiV1KeWHegRcUlEfCkiHo+IxyLi1m4WtlzbN6+lWglvjErqOwMd/O408LuZ+bWIWAc8HBH3ZubjXaptWYYGqlw2usYWuqS+s+wWembuzcyvtV4/D+wCLupWYZ24csuwk4sk9Z2u9KFHxDbgFcBDC/zslogYj4jxiYmJbhxuSTu2DvPswaMcOjq1KseTpHNBx4EeEWuBTwHvzMzT+jkyc2dmjmXm2OjoaKeHOyNXXdh8BMAjzxxcleNJ0rmgo0CPiBrNMP9kZn66OyV17qcu3UC9VuGBXft7XYokrZpORrkEcDuwKzM/1L2SOlevVXnd5aPct+sAmU4wktQfOmmhXwP8CnBtRDzS+npjl+rq2PU7NvHswaPeHJXUN5Y9bDEz/x2ILtbSVdfu2ATAfY/vZ0frsbqSVGalmyk6a9O6Oldfsp77dh/odSmStCpKG+gAb9ixif955iAHDk/2uhRJWnGlDvTrdmwG4AFb6ZL6QKkD/cot67hofYP7dhnoksqv1IEeEVy/YxP//uQEk1Mnel2OJK2oUgc6wPUv28zk1AxffGxfr0uRpBVV+kB/7WUb2b5pLX/6wJOuYiSp1Eof6NVKcOv123nywAt87hv/1+tyJGnFlD7QAd748q1csXkdH73/CVvpkkqrLwK90mqlf3viRf75f2ylSyqnvgh0gBuu2sKVW5qt9OkTM70uR5K6rm8CvVIJ3nn9dp76/ovc8R/f6XU5ktR1fRPoAD971RZuuGoL7/+X3Tyw22elSyqXvgr0iOBDv3g1V104zG/93dfZvc+FpCWVR18FOsB5gwPc9quvYs3QADffOc5+H9wlqST6LtABtozUue2mMZ578Thv+ui/8SUf3iWpBPoy0AF+4uL1fPY3r2Hj2iHecedX+b17HvN5L5IKrW8DHeDHNq/jM79xDe+4Zht3/ufTvOb99/PHX9zN3kNHe12aJJ21WM1FlMfGxnJ8fHzVjnc2vvr0c/zVg09x7679VCJ4zY9ewGsvv4BrLtvIVRcOM1Dt6799knooIh7OzLGl9lv2mqJl86ptG3jVtg0889wR/vah7/Ll3RP80Re+CXyTWjXYdsEaLhtdyyUbGmxaV2fT8BAb1gyyrl5j7dAAa4cGqNcq1GtVhgYqRJyzy61KKilb6IuYeP4YX3nqB+zae5gnD7zAtw+8wLMHj3JseumZpgOVoFatMFANqpWgGkFl9ns0h1BG0Pyi9ZrW9rkfFAu+7OofDP/0SCvvD37+x3nVtg3L+t1VaaFHxA3AR4AqcFtmfqCTzzvXjK4b4s1XX8ibr77wpW2ZyeHJaSaeP8YPjxzn+ckpnp+c5oVj00xOzTA5dYJj0zNMn5hheiY5Pj3DTCYnZpKZTGZmaH5PSJLM5mcmNF/POf7cP7an/Nnt4t/g7OaHSWqrUauu+DGWHegRUQX+HHgDsAf4akTck5mPd6u4c1FEMNKoMdKo9boUSTpFJ3f6fgp4MjOfyszjwN8Db+lOWZKks9VJoF8EPDPn/Z7WNklSD6z4WLyIuCUixiNifGJiYqUPJ0l9q5NAfxa4ZM77i1vbTpGZOzNzLDPHRkdHOzicJGkxnQT6V4HtEXFpRAwCbwPu6U5ZkqSztexRLpk5HRG/CXyR5rDFOzLzsa5VJkk6Kx2NQ8/MzwOf71ItkqQO+IASSSqJVZ36HxETwHeX+esbge93sZyi6Mfz7sdzhv487348Zzj78/6RzFxyVMmqBnonImL8TJ5lUDb9eN79eM7Qn+fdj+cMK3fedrlIUkkY6JJUEkUK9J29LqBH+vG8+/GcoT/Pux/PGVbovAvThy5JWlyRWuiSpEUUItAj4oaI+GZEPBkR7+51PSshIi6JiC9FxOMR8VhE3NraviEi7o2IJ1rfz+91rd0WEdWI+HpEfK71/tKIeKh1vf+h9WiJUomI9RFxd0TsjohdEfGasl/riPid1n/bj0bEXRFRL+O1jog7IuJARDw6Z9uC1zaaPto6/29ExCs7OfY5H+hzFtL4OeBlwNsj4mW9rWpFTAO/m5kvA14N/EbrPN8N3J+Z24H7W+/L5lZg15z3fwj8SWZeDvwQuLknVa2sjwBfyMwrgatpnn9pr3VEXAT8NjCWmS+n+biQt1HOa30ncMO8be2u7c8B21tftwAf6+TA53yg0ycLaWTm3sz8Wuv18zT/B7+I5rl+orXbJ4C39qbClRERFwNvAm5rvQ/gWuDu1i5lPOcR4PXA7QCZeTwzD1Lya03zUSONiBgAzgP2UsJrnZkPAs/N29zu2r4F+Ots+i9gfURsXe6xixDofbeQRkRsA14BPARszsy9rR/tAzb3qKyV8mHgXcDsytsXAAczc7r1vozX+1JgAvh4q6vptohYQ4mvdWY+C3wQ+B7NID8EPEz5r/Wsdte2q/lWhEDvKxGxFvgU8M7MPDz3Z9kcklSaYUkRcSNwIDMf7nUtq2wAeCXwscx8BfAi87pXSnitz6fZGr0UuBBYw+ndEn1hJa9tEQL9jBbSKIOIqNEM809m5qdbm/fP/hOs9f1Ar+pbAdcAb46Ip2l2pV1Ls295feuf5VDO670H2JOZD7Xe300z4Mt8ra8HvpOZE5k5BXya5vUv+7We1e7adjXfihDofbGQRqvv+HZgV2Z+aM6P7gFuar2+Cfjsate2UjLzPZl5cWZuo3ldH8jMXwK+BPxCa7dSnTNAZu4DnomIK1qbrgMep8TXmmZXy6sj4rzWf+uz51zqaz1Hu2t7D/CrrdEurwYOzemaOXuZec5/AW8EvgV8G3hvr+tZoXN8Hc1/hn0DeKT19Uaafcr3A08A9wEbel3rCp3/zwCfa73+UeC/gSeBfwSGel3fCpzvTwLjrev9GeD8sl9r4PeB3cCjwN8AQ2W81sBdNO8TTNH819jN7a4tEDRH8X0b+F+ao4CWfWxnikpSSRShy0WSdAYMdEkqCQNdkkrCQJekkjDQJakkDHRJKgkDXZJKwkCXpJL4f6PQF2JF/bpHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ロジスティック回帰モデル "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data ======================\n",
      " [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "target ======================\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "target_names ======================\n",
      " ['setosa' 'versicolor' 'virginica']\n",
      "DESCR ======================\n",
      " .. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n",
      "feature_names ======================\n",
      " ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "filename ======================\n",
      " /usr/local/lib/python3.6/site-packages/sklearn/datasets/data/iris.csv\n"
     ]
    }
   ],
   "source": [
    "#今回用いるirisデータセットのロードと中身の確認\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "for i in iris:\n",
    "    print(i,\"======================\\n\",iris[i])\n",
    "\n",
    "#irisデータセットは0,1,2の3クラスなので、今回は0,1の2クラスのデータのみを利用\n",
    "X = iris.data[0:100]\n",
    "y = iris.target[0:100]\n",
    "#numpyのndarrayをpytorchのtensorに変換\n",
    "X = torch.tensor(X, dtype = torch.float32)\n",
    "y = torch.tensor(y, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデルの構築\n",
    "net = nn.Linear(4, 1)\n",
    "\n",
    "#シグモイド関数を作用させ、2クラス分類のクロスエントロピーを計算する関数\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "#SGD\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#損失関数のログ\n",
    "losses = []\n",
    "\n",
    "#100回イテレーションを回す\n",
    "for epoc in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = net(X)\n",
    "    loss = loss_fn(y_pred.view_as(y),y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8496102690696716,\n",
       " 1.0882059335708618,\n",
       " 0.633676290512085,\n",
       " 0.7638390064239502,\n",
       " 0.4604349434375763,\n",
       " 0.46952176094055176,\n",
       " 0.2858083248138428,\n",
       " 0.24176645278930664,\n",
       " 0.17795133590698242,\n",
       " 0.15804241597652435,\n",
       " 0.1459852010011673,\n",
       " 0.13827574253082275,\n",
       " 0.13174651563167572,\n",
       " 0.12586820125579834,\n",
       " 0.1204976737499237,\n",
       " 0.11556778103113174,\n",
       " 0.11102678626775742,\n",
       " 0.10683099925518036,\n",
       " 0.10294288396835327,\n",
       " 0.0993301272392273,\n",
       " 0.09596476703882217,\n",
       " 0.09282242506742477,\n",
       " 0.08988180011510849,\n",
       " 0.08712423592805862,\n",
       " 0.0845332071185112,\n",
       " 0.08209418505430222,\n",
       " 0.07979424297809601,\n",
       " 0.07762183248996735,\n",
       " 0.07556672394275665,\n",
       " 0.07361966371536255,\n",
       " 0.07177237421274185,\n",
       " 0.07001738995313644,\n",
       " 0.0683479905128479,\n",
       " 0.06675806641578674,\n",
       " 0.06524209678173065,\n",
       " 0.06379503011703491,\n",
       " 0.06241227313876152,\n",
       " 0.06108963117003441,\n",
       " 0.059823278337717056,\n",
       " 0.058609675616025925,\n",
       " 0.05744563043117523,\n",
       " 0.05632810667157173,\n",
       " 0.055254414677619934,\n",
       " 0.05422200262546539,\n",
       " 0.05322851985692978,\n",
       " 0.05227182060480118,\n",
       " 0.05134988948702812,\n",
       " 0.050460848957300186,\n",
       " 0.04960298165678978,\n",
       " 0.04877464845776558,\n",
       " 0.04797437787055969,\n",
       " 0.04720074310898781,\n",
       " 0.04645242169499397,\n",
       " 0.04572818800806999,\n",
       " 0.04502691328525543,\n",
       " 0.04434751346707344,\n",
       " 0.043688975274562836,\n",
       " 0.04305032268166542,\n",
       " 0.04243069142103195,\n",
       " 0.04182925075292587,\n",
       " 0.04124517738819122,\n",
       " 0.04067773371934891,\n",
       " 0.040126219391822815,\n",
       " 0.039589982479810715,\n",
       " 0.039068371057510376,\n",
       " 0.03856080770492554,\n",
       " 0.03806672245264053,\n",
       " 0.03758558630943298,\n",
       " 0.0371168851852417,\n",
       " 0.03666013851761818,\n",
       " 0.0362149141728878,\n",
       " 0.035780757665634155,\n",
       " 0.035357266664505005,\n",
       " 0.034944046288728714,\n",
       " 0.03454071655869484,\n",
       " 0.034146953374147415,\n",
       " 0.033762384206056595,\n",
       " 0.033386703580617905,\n",
       " 0.03301962465047836,\n",
       " 0.03266081213951111,\n",
       " 0.032310016453266144,\n",
       " 0.03196696564555168,\n",
       " 0.03163140267133713,\n",
       " 0.03130307421088219,\n",
       " 0.03098176419734955,\n",
       " 0.03066723607480526,\n",
       " 0.030359266325831413,\n",
       " 0.030057668685913086,\n",
       " 0.02976224198937416,\n",
       " 0.029472792521119118,\n",
       " 0.02918914146721363,\n",
       " 0.02891111932694912,\n",
       " 0.028638551011681557,\n",
       " 0.02837126888334751,\n",
       " 0.028109131380915642,\n",
       " 0.027852008119225502,\n",
       " 0.027599716559052467,\n",
       " 0.027352148666977882,\n",
       " 0.02710914798080921,\n",
       " 0.026870619505643845]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.0485],\n",
       "        [-3.1554],\n",
       "        [-3.6573],\n",
       "        [-3.0345],\n",
       "        [-4.1921],\n",
       "        [-3.9827],\n",
       "        [-3.6692],\n",
       "        [-3.6403],\n",
       "        [-2.8618],\n",
       "        [-3.1951],\n",
       "        [-4.2461],\n",
       "        [-3.3757],\n",
       "        [-3.2161],\n",
       "        [-3.7350],\n",
       "        [-5.4957],\n",
       "        [-5.3300],\n",
       "        [-4.8411],\n",
       "        [-3.9628],\n",
       "        [-3.9748],\n",
       "        [-4.2541],\n",
       "        [-3.3110],\n",
       "        [-3.9997],\n",
       "        [-4.9506],\n",
       "        [-2.8103],\n",
       "        [-2.7319],\n",
       "        [-2.7512],\n",
       "        [-3.2543],\n",
       "        [-3.8589],\n",
       "        [-3.9049],\n",
       "        [-3.0135],\n",
       "        [-2.8698],\n",
       "        [-3.5688],\n",
       "        [-4.9564],\n",
       "        [-5.3288],\n",
       "        [-3.1094],\n",
       "        [-3.9468],\n",
       "        [-4.3630],\n",
       "        [-4.2529],\n",
       "        [-3.2451],\n",
       "        [-3.6653],\n",
       "        [-4.1524],\n",
       "        [-2.0040],\n",
       "        [-3.5823],\n",
       "        [-3.2515],\n",
       "        [-3.3100],\n",
       "        [-3.0447],\n",
       "        [-4.1252],\n",
       "        [-3.4177],\n",
       "        [-4.2211],\n",
       "        [-3.6863],\n",
       "        [ 4.0929],\n",
       "        [ 3.8993],\n",
       "        [ 4.8014],\n",
       "        [ 4.3974],\n",
       "        [ 4.7635],\n",
       "        [ 4.5773],\n",
       "        [ 4.2706],\n",
       "        [ 2.6194],\n",
       "        [ 4.3984],\n",
       "        [ 3.6690],\n",
       "        [ 3.6981],\n",
       "        [ 3.7177],\n",
       "        [ 4.1840],\n",
       "        [ 4.8237],\n",
       "        [ 2.5023],\n",
       "        [ 3.6927],\n",
       "        [ 4.4365],\n",
       "        [ 3.6054],\n",
       "        [ 5.6356],\n",
       "        [ 3.6492],\n",
       "        [ 4.9252],\n",
       "        [ 3.4044],\n",
       "        [ 5.9631],\n",
       "        [ 4.8209],\n",
       "        [ 3.8046],\n",
       "        [ 3.8863],\n",
       "        [ 5.0320],\n",
       "        [ 5.4060],\n",
       "        [ 4.5052],\n",
       "        [ 2.5115],\n",
       "        [ 3.6282],\n",
       "        [ 3.3279],\n",
       "        [ 3.3477],\n",
       "        [ 6.2157],\n",
       "        [ 4.4865],\n",
       "        [ 3.7477],\n",
       "        [ 4.4222],\n",
       "        [ 5.0559],\n",
       "        [ 3.4067],\n",
       "        [ 4.0602],\n",
       "        [ 4.6642],\n",
       "        [ 4.4404],\n",
       "        [ 3.7309],\n",
       "        [ 2.7630],\n",
       "        [ 4.1271],\n",
       "        [ 3.5106],\n",
       "        [ 3.7649],\n",
       "        [ 3.8546],\n",
       "        [ 1.8427],\n",
       "        [ 3.7189]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深層学習モデル定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 記法1\n",
    "kerasなどと同様の書き方ができます<br>\n",
    "各層に名前をつける事ができます(下の例だと'fc1','relu','fc2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential()\n",
    "model.add_module('fc1', nn.Linear(10,100))\n",
    "model.add_module('relu', nn.ReLU())\n",
    "model.add_module('fc2', nn.Linear(100,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 記法2\n",
    "層をリストにまとめてモデルに挿入することもできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = []\n",
    "layer.append(nn.Linear(10,100))\n",
    "layer.append(nn.ReLU())\n",
    "layer.append(nn.Linear(100,10))\n",
    "model = nn.Sequential(*layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 記法3\n",
    "クラスとして定義することも可能です<br>\n",
    "以下、主にこの記法に基づいて説明します."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        #層を表す変数について定義\n",
    "        super(Model,self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self,x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "model = Model()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "params = list(model.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1135, -0.2168, -0.0346,  0.0507, -0.0894, -0.0497,  0.1061,  0.0091,\n",
      "         -0.0920,  0.0105]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input_model = torch.randn(1, 1, 32, 32)\n",
    "out = model(input_model)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### リスト記法の注意点\n",
    "レイヤーをリストで保持した際は、同じハイパーパラメータのレイヤーを何度も使う際にはfor文を使って定義すれば楽なのですが，<br>\n",
    "学習可能なパラメータをリストで保持してしまうと，<br>\n",
    "モデルのパラメータを呼び出すときにリストで保持しているレイヤーのパラメータはパラメータとして認識されず呼び出されません．<br>\n",
    "そうすると何が起こるかというと，学習の時にパラメータの更新が行われなくってしまいます．<br>\n",
    "以下悪い例です．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-231-47fc509848e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# listで保持しているとlist内のモジュールのパラメータは取得できない\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# optimについては後述\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0moptimize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, momentum, dampening, weight_decay, nesterov)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdampening\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Nesterov momentum requires a momentum and zero dampening\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"optimizer got an empty parameter list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer = [nn.Linear(10,10) for _ in range(10)]\n",
    "        #ここでレイヤーをリストで表現.\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layer)):\n",
    "            x = self.layer[i](x)\n",
    "        return x\n",
    "\n",
    "model = Model()\n",
    "# model.parameters()で学習パラメータのイテレータを取得できるが，\n",
    "# listで保持しているとlist内のモジュールのパラメータは取得できない\n",
    "# optimについては後述\n",
    "optimize = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そのためこのような場合にはnn.ModuleListを使って定義します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        layer = [nn.Linear(10,10) for _ in range(10)]\n",
    "        self.layer = nn.ModuleList(layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layer)):\n",
    "            x = self.layer[i](x)\n",
    "        return x\n",
    "\n",
    "model = Model()\n",
    "# model.parameters()で学習パラメータのイテレータを取得できるが，\n",
    "# listで保持しているとlist内のモジュールのパラメータは取得できない\n",
    "# optimについては後述\n",
    "optimize = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-237-d2c238979a37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#ランダムな勾配でback-propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "#勾配を初期化\n",
    "model.zero_grad()\n",
    "\n",
    "#ランダムな勾配でback-propagation\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fine tuning\n",
    "pytorchではtorchvision.modelsを使うことで<br>\n",
    "AlexNet、VGGNet、ResNet、DenseNet、SqueezeNet、GoogleNetが簡単に定義可能で、<br>\n",
    "また、これらの学習済みモデルを簡単に使用することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "resnet = models.resnet50(pretrained=True) #PretrainedオプションをTrueにすることで学習済みモデルをダウンロード可能\n",
    "resnet.fc = nn.Linear(2048, 100) #このように、層の名前を指定して出力を変更できます\n",
    "resnet2 = nn.Sequential(*list(resnet.children())[:-3])#また、一部の層のみを使いたい場合はこののように記述できます\n",
    "#重みの学習をしたくない場合は以下のように各パラメータのrequire_gradをFalseにすることで重みを固定することができます\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n",
    "optimパッケージを使って任意の最適化手法を用いてパラメータの更新を行います。<br>\n",
    "最適化手法のパラメータを設定したら、backward計算をするたびstep()を呼び出すことで更新を行うことができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 損失関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(38.6841, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#input->net->output\n",
    "output = model(input_model)\n",
    "\n",
    "#ラベル情報\n",
    "target = torch.range(1, 10)  # a dummy target, for example\n",
    "\n",
    "#loss関数の定義\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "#どの部分でlossの比較をするか\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "None\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-238-bc111b448ac1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#lossに基づくback propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#back propagation後\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "#勾配を初期化\n",
    "model.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "#back propagation前\n",
    "print('conv1.bias.grad before backward')\n",
    "print(model.conv1.bias.grad) #conv1層のbiasの勾配\n",
    "\n",
    "#lossに基づくback propagation\n",
    "loss.backward()\n",
    "\n",
    "#back propagation後\n",
    "print('conv1.bias.grad after backward')\n",
    "print(model.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pytorch 0.4 以降\n",
    "if torch.cuda.is_available(): # GPUが利用可能か確認\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "# modelの定義\n",
    "model = models.resnet18()\n",
    "model = model.to(device)\n",
    "# 最適化手法のパラメータ設定\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "# loss関数の定義\n",
    "criterion = nn.MSELoss()\n",
    "# 入力と正解を乱数で生成\n",
    "input = torch.randn(1,3,224,224) # バッチ x チャネル x 高さ x 幅\n",
    "target = torch.randn(1,1000)\n",
    "target.requires_grad = False\n",
    "# バッチ正規化等、学習時と推論時で振る舞いの違うモジュールの振る舞いを学習時の振る舞いに\n",
    "# model.eval()で推論時の振る舞いに変更可能\n",
    "model.train()\n",
    "# 学習ループ\n",
    "for i in range(100):\n",
    "  # 順伝播\n",
    "  out = model(input)\n",
    "  # ロスの計算\n",
    "  loss = criterion(out, target)\n",
    "  # 勾配の初期化\n",
    "  optimizer.zero_grad()\n",
    "  # 勾配の計算\n",
    "  loss.backward()\n",
    "  # パラメータの更新\n",
    "  optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss関数の指定\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#Optimizerの指定\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-239-befc21d3a14e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#入力データ・ラベルに分割\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainloader' is not defined"
     ]
    }
   ],
   "source": [
    "#トレーニング\n",
    "#エポック数の指定\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    #データ全てのトータルロス\n",
    "    running_loss = 0.0 \n",
    "    for i, data in enumerate(trainloader):\n",
    "\n",
    "        #入力データ・ラベルに分割\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Variableに変形\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # optimizerの初期化\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #一連の流れ\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        #ここでラベルデータに対するCross-Entropyがとられる\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ロスの表示\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの保存と読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデルの保存\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "#モデルの読み込み\n",
    "param = torch.load('model.pth')\n",
    "model = Model() #読み込む前にクラス宣言が必要\n",
    "#model = Net()\n",
    "model.load_state_dict(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## カスタマイズ\n",
    "### 自作活性化関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyReLU(torch.autograd.Function):\n",
    "\n",
    "    #forwardの活性化関数とbackwardの計算のみ記述すれば良い\n",
    "    def forward(self, input):\n",
    "\n",
    "        #値の記憶\n",
    "        self.save_for_backward(input)\n",
    "\n",
    "        #ReLUの定義部分\n",
    "        #x.clamp(min=0) <=> max(x, 0)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    #backpropagationの記述\n",
    "    #勾配情報を返せば良い\n",
    "    def backward(self, grad_output):\n",
    "\n",
    "        #記憶したTensorの呼び出し\n",
    "        input, = self.saved_tensors\n",
    "\n",
    "        #参照渡しにならないようコピー\n",
    "        grad_input = grad_output.clone()\n",
    "\n",
    "        #input<0 => 0  else input\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自作ロス関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletMarginLoss(nn.Module):\n",
    "    def __init__(self, margin):\n",
    "        super(TripletMarginLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        dist = torch.sum(\n",
    "            torch.pow((anchor - positive),2) - torch.pow((anchor - negative),2),\n",
    "            dim=1) + self.margin\n",
    "        dist_hinge = torch.clamp(dist, min=0.0)  #max(dist, 0.0)と等価\n",
    "        loss = torch.mean(dist_hinge)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ダイナミックネットワーク\n",
    "defined-by-run形式なので、forward部分に条件分岐などしてレイヤーを組み替えることもできる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicNet(torch.nn.Module):\n",
    "\n",
    "    #層の定義\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    #ランダムに中間層を0~3に変更する\n",
    "    def forward(self, x):\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        for _ in range(random.randint(0, 3)):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.ones(2, 2), requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, 2, requires_grad = True)\n",
    "x.data\n",
    "x.grad\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3.],\n",
       "        [3., 3.]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.data[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-6546668eaede>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mtotal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \"\"\"\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 0"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "latent_size = 64\n",
    "hidden_size = 256\n",
    "image_size = 784\n",
    "num_epochs = 200\n",
    "batch_size = 100\n",
    "sample_dir = 'samples'\n",
    "\n",
    "# Image processing\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=(0.5),   # 3 for RGB channels\n",
    "                                                     std=(0.5))])\n",
    "\n",
    "# MNIST dataset\n",
    "mnist = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                   train=True,\n",
    "                                   transform=transform,\n",
    "                                   download=True)\n",
    "\n",
    "# Data loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True)\n",
    "\n",
    "# Discriminator\n",
    "D = nn.Sequential(\n",
    "    nn.Linear(image_size, hidden_size),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(hidden_size, 1),\n",
    "    nn.Sigmoid())\n",
    "\n",
    "# Generator \n",
    "G = nn.Sequential(\n",
    "    nn.Linear(latent_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, image_size),\n",
    "    nn.Tanh())\n",
    "\n",
    "# Device setting\n",
    "D = D.to(device)\n",
    "G = G.to(device)\n",
    "\n",
    "# Binary cross entropy loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=0.0002)\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0002)\n",
    "\n",
    "def denorm(x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp(0, 1)\n",
    "\n",
    "def reset_grad():\n",
    "    d_optimizer.zero_grad()\n",
    "    g_optimizer.zero_grad()\n",
    "    \n",
    "# Start training\n",
    "total_step = len(data_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, _) in enumerate(data_loader):\n",
    "        images = images.reshape(batch_size, -1).to(device)\n",
    "        \n",
    "        # Create the labels which are later used as input for the BCE loss\n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "        # ================================================================== #\n",
    "        #                      Train the discriminator                       #\n",
    "        # ================================================================== #\n",
    "\n",
    "        # Compute BCE_Loss using real images where BCE_Loss(x, y): - y * log(D(x)) - (1-y) * log(1 - D(x))\n",
    "        # Second term of the loss is always zero since real_labels == 1\n",
    "        outputs = D(images)\n",
    "        d_loss_real = criterion(outputs, real_labels)\n",
    "        real_score = outputs\n",
    "        \n",
    "        # Compute BCELoss using fake images\n",
    "        # First term of the loss is always zero since fake_labels == 0\n",
    "        z = torch.randn(batch_size, latent_size).to(device)\n",
    "        fake_images = G(z)\n",
    "        outputs = D(fake_images)\n",
    "        d_loss_fake = criterion(outputs, fake_labels)\n",
    "        fake_score = outputs\n",
    "        \n",
    "        # Backprop and optimize\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        reset_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # ================================================================== #\n",
    "        #                        Train the generator                         #\n",
    "        # ================================================================== #\n",
    "\n",
    "        # Compute loss with fake images\n",
    "        z = torch.randn(batch_size, latent_size).to(device)\n",
    "        fake_images = G(z)\n",
    "        outputs = D(fake_images)\n",
    "        \n",
    "        # We train G to maximize log(D(G(z)) instead of minimizing log(1-D(G(z)))\n",
    "        # For the reason, see the last paragraph of section 3. https://arxiv.org/pdf/1406.2661.pdf\n",
    "        g_loss = criterion(outputs, real_labels)\n",
    "        \n",
    "        # Backprop and optimize\n",
    "        reset_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        if (i+1) % 200 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}' \n",
    "                  .format(epoch, num_epochs, i+1, total_step, d_loss.item(), g_loss.item(), \n",
    "                          real_score.mean().item(), fake_score.mean().item()))\n",
    "    \n",
    "    # Save real images\n",
    "    if (epoch+1) == 1:\n",
    "        images = images.reshape(images.size(0), 1, 28, 28)\n",
    "        save_image(denorm(images), os.path.join(sample_dir, 'real_images.png'))\n",
    "    \n",
    "    # Save sampled images\n",
    "    fake_images = fake_images.reshape(fake_images.size(0), 1, 28, 28)\n",
    "    save_image(denorm(fake_images), os.path.join(sample_dir, 'fake_images-{}.png'.format(epoch+1)))\n",
    "\n",
    "# Save the model checkpoints \n",
    "torch.save(G.state_dict(), 'G.ckpt')\n",
    "torch.save(D.state_dict(), 'D.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [100/600], Loss: 0.5952\n",
      "Epoch [1/2], Step [200/600], Loss: 0.2299\n",
      "Epoch [1/2], Step [300/600], Loss: 0.1024\n",
      "Epoch [1/2], Step [400/600], Loss: 0.2094\n",
      "Epoch [1/2], Step [500/600], Loss: 0.1041\n",
      "Epoch [1/2], Step [600/600], Loss: 0.0315\n",
      "Epoch [2/2], Step [100/600], Loss: 0.1303\n",
      "Epoch [2/2], Step [200/600], Loss: 0.0844\n",
      "Epoch [2/2], Step [300/600], Loss: 0.0899\n",
      "Epoch [2/2], Step [400/600], Loss: 0.0348\n",
      "Epoch [2/2], Step [500/600], Loss: 0.0740\n",
      "Epoch [2/2], Step [600/600], Loss: 0.2158\n",
      "Test Accuracy of the model on the 10000 test images: 97.37 %\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "sequence_length = 28\n",
    "input_size = 28\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "num_epochs = 2\n",
    "learning_rate = 0.01\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# Recurrent neural network (many-to-one)\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) \n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
